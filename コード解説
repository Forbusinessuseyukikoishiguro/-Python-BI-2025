# =============================================================================
# Pythonデータクリーニング：1行ずつ詳細解説
# 新人エンジニア向けガイド
# =============================================================================

# 【ライブラリのインポート】
import pandas as pd              # データ操作のメインライブラリ（DataFrameを扱うため）
import numpy as np              # 数値計算ライブラリ（数学的操作やNaN処理に使用）
import warnings                 # 警告メッセージを制御するライブラリ
warnings.filterwarnings('ignore')  # 不要な警告を非表示にする（作業効率向上のため）

# 【データの読み込み】
# CSVファイルを読み込んでDataFrameオブジェクトに格納
df = pd.read_csv('sample_data.csv')  # ファイルパスを指定してCSVを読み込み

# 【基本情報の確認】
print("=" * 50)              # 区切り線を出力（視認性向上のため）
print("データの基本情報")        # セクションタイトルを出力
print("=" * 50)              # 区切り線を出力

print(f"データの形状: {df.shape}")    # (行数, 列数)を表示
print(f"列名: {list(df.columns)}")   # 全ての列名をリスト形式で表示
print("\n")                          # 改行を追加（読みやすさのため）

# データ型を確認（文字列、数値、日付などの型を把握）
print("各列のデータ型:")
print(df.dtypes)                     # 各列のデータ型を一覧表示
print("\n")

# 最初の5行を表示（データの中身を確認）
print("データの最初の5行:")
print(df.head())                     # head()メソッドでデータの先頭を確認
print("\n")

# 欠損値の状況を確認
print("欠損値の確認:")
missing_info = df.isnull().sum()     # 各列の欠損値数を計算
print(missing_info)                  # 欠損値数を表示
print(f"全体の欠損値数: {missing_info.sum()}")  # 全ての欠損値の合計
print("\n")

# 重複行の確認
duplicate_count = df.duplicated().sum()  # 重複行の数をカウント
print(f"重複行数: {duplicate_count}")      # 重複行数を表示
print("\n")

# =============================================================================
# 【STEP 1: 列名の標準化】
# =============================================================================

print("STEP 1: 列名の標準化")
print("-" * 30)

# 元の列名を保存（後で比較できるように）
original_columns = df.columns.tolist()  # 元の列名をリストとして保存

# 列名を小文字に変換
df.columns = df.columns.str.lower()     # str.lower()で全て小文字に変換

# 空白をアンダースコアに置換
df.columns = df.columns.str.replace(' ', '_')  # スペースを_に置換

# 特殊文字を除去（英数字とアンダースコア以外を削除）
df.columns = df.columns.str.replace('[^a-zA-Z0-9_]', '', regex=True)

print("列名の変更:")
for old, new in zip(original_columns, df.columns):  # 元の名前と新しい名前をペアで表示
    if old != new:                      # 変更があった場合のみ表示
        print(f"  {old} → {new}")       # 変更内容を表示
print("\n")

# =============================================================================
# 【STEP 2: 重複行の削除】
# =============================================================================

print("STEP 2: 重複行の削除")
print("-" * 30)

initial_rows = len(df)                  # 削除前の行数を記録
df = df.drop_duplicates()               # 重複行を削除（完全に同じ行を削除）
final_rows = len(df)                    # 削除後の行数を記録
removed_rows = initial_rows - final_rows # 削除された行数を計算

print(f"削除前: {initial_rows}行")       # 削除前の行数を表示
print(f"削除後: {final_rows}行")         # 削除後の行数を表示
print(f"削除された行数: {removed_rows}行") # 削除された行数を表示
print("\n")

# =============================================================================
# 【STEP 3: データ型の変換】
# =============================================================================

print("STEP 3: データ型の変換")
print("-" * 30)

# 日付列の自動検出と変換
for column in df.columns:               # 全ての列を順番に確認
    # 列名に'date'または'time'が含まれている場合
    if 'date' in column or 'time' in column:
        try:                            # エラーが発生する可能性があるのでtry-except文を使用
            # 日付型に変換を試行
            df[column] = pd.to_datetime(df[column])
            print(f"✅ {column}を日付型に変換しました")
        except ValueError as e:         # 変換エラーが発生した場合
            print(f"❌ {column}の日付変換に失敗: {e}")
        except Exception as e:          # その他のエラーが発生した場合
            print(f"❌ {column}で予期しないエラー: {e}")

# 数値列の変換（文字列として読み込まれた数値を変換）
for column in df.select_dtypes(include=['object']).columns:  # オブジェクト型（文字列）の列のみ対象
    try:
        # 文字列を一度stringに変換してから数値変換前の前処理
        cleaned_series = df[column].astype(str)  # 確実に文字列型にする
        
        # カンマ、円マーク、ドルマークを除去
        cleaned_series = cleaned_series.str.replace(',', '')     # カンマを除去
        cleaned_series = cleaned_series.str.replace('¥', '')     # 円マークを除去
        cleaned_series = cleaned_series.str.replace('$', '')     # ドルマークを除去
        cleaned_series = cleaned_series.str.replace(' ', '')     # 空白を除去
        
        # 数値型に変換を試行
        df[column] = pd.to_numeric(cleaned_series, errors='coerce')  # errors='coerce'で変換失敗時はNaN
        
        # NaNでない値が存在すれば変換成功とみなす
        if not df[column].isna().all():  # 全てがNaNでない場合
            print(f"✅ {column}を数値型に変換しました")
        else:                           # 全てがNaNの場合は元に戻す
            df[column] = df[column].astype(str)  # 元の文字列型に戻す
            
    except Exception as e:              # 変換エラーの場合
        print(f"ℹ️ {column}は文字列のまま保持します")
        continue                        # 次の列の処理に続行

print("\n")

# =============================================================================
# 【STEP 4: 欠損値の処理】
# =============================================================================

print("STEP 4: 欠損値の処理")
print("-" * 30)

# 処理前の欠損値数を記録
missing_before = df.isnull().sum().sum()  # 全体の欠損値数を計算

# 数値列の欠損値を平均値で補完
numeric_columns = df.select_dtypes(include=[np.number]).columns  # 数値型の列を取得
for column in numeric_columns:          # 数値列を順番に処理
    if df[column].isnull().sum() > 0:   # 欠損値が存在する場合のみ処理
        mean_value = df[column].mean()  # 平均値を計算
        df[column].fillna(mean_value, inplace=True)  # 平均値で欠損値を補完
        print(f"✅ {column}: 平均値 {mean_value:.2f} で補完")

# カテゴリ列（文字列列）の欠損値を最頻値で補完
categorical_columns = df.select_dtypes(include=['object', 'category']).columns  # 文字列/カテゴリ型の列
for column in categorical_columns:      # カテゴリ列を順番に処理
    if df[column].isnull().sum() > 0:   # 欠損値が存在する場合のみ処理
        mode_values = df[column].mode() # 最頻値を取得（複数ある場合もある）
        if len(mode_values) > 0:        # 最頻値が存在する場合
            fill_value = mode_values[0] # 最初の最頻値を使用
            df[column].fillna(fill_value, inplace=True)  # 最頻値で補完
            print(f"✅ {column}: 最頻値 '{fill_value}' で補完")
        else:                           # 最頻値が取得できない場合
            df[column].fillna('Unknown', inplace=True)   # 'Unknown'で補完
            print(f"✅ {column}: 'Unknown' で補完")

# 日付列の欠損値処理
datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns  # 日付型の列
for column in datetime_columns:         # 日付列を順番に処理
    if df[column].isnull().sum() > 0:   # 欠損値が存在する場合
        # 前の値で補完（forward fill）
        df[column].fillna(method='ffill', inplace=True)
        # まだ欠損値がある場合は後の値で補完（backward fill）
        df[column].fillna(method='bfill', inplace=True)
        print(f"✅ {column}: 前後の値で補完")

# 処理後の欠損値数を確認
missing_after = df.isnull().sum().sum()   # 処理後の欠損値数
print(f"\n欠損値処理結果:")
print(f"  処理前: {missing_before}個")    # 処理前の欠損値数
print(f"  処理後: {missing_after}個")     # 処理後の欠損値数
print(f"  補完済み: {missing_before - missing_after}個")  # 補完した欠損値数
print("\n")

# =============================================================================
# 【STEP 5: 異常値の検出と処理】
# =============================================================================

print("STEP 5: 異常値の検出と処理")
print("-" * 30)

# 数値列に対して異常値を検出（IQR法を使用）
for column in numeric_columns:          # 数値列を順番に処理
    Q1 = df[column].quantile(0.25)      # 第1四分位数（25%点）
    Q3 = df[column].quantile(0.75)      # 第3四分位数（75%点）
    IQR = Q3 - Q1                       # 四分位範囲（IQR）を計算
    
    # 異常値の閾値を計算
    lower_bound = Q1 - 1.5 * IQR        # 下限閾値
    upper_bound = Q3 + 1.5 * IQR        # 上限閾値
    
    # 異常値を検出
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    outlier_count = len(outliers)       # 異常値の数をカウント
    
    if outlier_count > 0:               # 異常値が存在する場合
        print(f"⚠️ {column}: {outlier_count}個の異常値を検出")
        print(f"   正常範囲: {lower_bound:.2f} ～ {upper_bound:.2f}")
        
        # 異常値を上下限値でクリッピング（外れ値を閾値に置き換え）
        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
        print(f"   → 閾値でクリッピングしました")
    else:
        print(f"✅ {column}: 異常値は検出されませんでした")

print("\n")

# =============================================================================
# 【STEP 6: Tableau用最適化】
# =============================================================================

print("STEP 6: Tableau用最適化")
print("-" * 30)

# 日付列から追加の時間軸列を作成
datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns
for column in datetime_columns:         # 日付列を順番に処理
    base_name = column.replace('_date', '').replace('_time', '')  # ベース名を生成
    
    # 年列を作成
    df[f'{base_name}_year'] = df[column].dt.year
    print(f"✅ {base_name}_year 列を作成")
    
    # 月列を作成
    df[f'{base_name}_month'] = df[column].dt.month
    print(f"✅ {base_name}_month 列を作成")
    
    # 四半期列を作成
    df[f'{base_name}_quarter'] = df[column].dt.quarter
    print(f"✅ {base_name}_quarter 列を作成")
    
    # 曜日列を作成
    df[f'{base_name}_weekday'] = df[column].dt.day_name()
    print(f"✅ {base_name}_weekday 列を作成")
    
    # 月名列を作成
    df[f'{base_name}_month_name'] = df[column].dt.month_name()
    print(f"✅ {base_name}_month_name 列を作成")

# カテゴリ型への変換（メモリ使用量削減）
for column in df.select_dtypes(include=['object']).columns:  # 文字列型の列
    unique_ratio = df[column].nunique() / len(df)  # ユニーク値の割合を計算
    if unique_ratio < 0.5:              # ユニーク値が50%未満の場合
        df[column] = df[column].astype('category')  # カテゴリ型に変換
        print(f"✅ {column} をカテゴリ型に変換（メモリ削減）")

print("\n")

# =============================================================================
# 【STEP 7: 最終確認とエクスポート】
# =============================================================================

print("STEP 7: 最終確認とエクスポート")
print("-" * 30)

# 最終的なデータの概要を表示
print("🎉 データクリーニング完了！")
print(f"最終データ形状: {df.shape}")      # 最終的な(行数, 列数)
print(f"最終欠損値数: {df.isnull().sum().sum()}")  # 最終的な欠損値数

# データ型の分布を表示
print("\nデータ型の分布:")
dtype_counts = df.dtypes.value_counts()  # データ型ごとの列数をカウント
for dtype, count in dtype_counts.items():  # データ型と列数を表示
    print(f"  {dtype}: {count}列")

# メモリ使用量を表示
memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB単位に変換
print(f"\nメモリ使用量: {memory_usage:.2f} MB")

# クリーニング済みデータをCSVで保存
output_filename = 'cleaned_data.csv'    # 出力ファイル名を定義
df.to_csv(output_filename, index=False, encoding='utf-8')  # CSVファイルとして保存
print(f"\n💾 クリーニング済みデータを '{output_filename}' として保存しました")

# 最終データの先頭5行を表示
print("\n📊 最終データの先頭5行:")
print(df.head())                        # クリーニング後のデータを確認

print("\n" + "=" * 50)
print("🚀 全ての処理が完了しました！")
print("=" * 50)
