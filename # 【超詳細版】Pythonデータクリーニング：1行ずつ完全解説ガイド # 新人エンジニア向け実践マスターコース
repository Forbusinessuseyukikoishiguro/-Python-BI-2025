# =============================================================================
# 【超詳細版】Pythonデータクリーニング：1行ずつ完全解説ガイド
# 新人エンジニア向け実践マスターコース
# =============================================================================

# 📚 このスクリプトの目的と価値
# ・生データを分析可能な形に変換する
# ・Tableauでのビジュアライゼーションに最適化する
# ・データ品質を向上させて信頼性の高い分析を実現する
# ・手作業を自動化して効率性を大幅に向上させる

print("🚀 Pythonデータクリーニング：プロフェッショナル版")
print("=" * 60)

# =============================================================================
# 【セクション1: ライブラリのインポートと環境設定】
# =============================================================================

# pandas: データ操作の中核ライブラリ（Excel操作のPython版のようなもの）
import pandas as pd

# numpy: 数値計算とNaN（欠損値）処理に特化したライブラリ
import numpy as np

# datetime: 日付・時刻操作用（データ分析では日付処理が頻繁）
import datetime as dt
from datetime import datetime

# warnings: Python実行時の警告メッセージを制御
import warnings
# ignore: 分析作業中の不要な警告を非表示にして集中力を維持
warnings.filterwarnings('ignore')

# logging: 処理の記録を残すためのライブラリ（プロダクション環境では必須）
import logging

# os: ファイルやディレクトリ操作（ファイルパス処理など）
import os

# sys: Pythonシステム関連の操作（メモリ情報取得など）
import sys

print("✅ 必要ライブラリの読み込み完了")

# ログ設定（処理の記録を残すプロフェッショナルな手法）
logging.basicConfig(
    level=logging.INFO,                    # INFO レベル以上のログを出力
    format='%(asctime)s - %(levelname)s - %(message)s',  # ログの形式を定義
    handlers=[
        logging.FileHandler('data_cleaning.log'),  # ファイルにログを保存
        logging.StreamHandler()                     # コンソールにもログを表示
    ]
)
# ロガーオブジェクトを作成（以降のログ出力で使用）
logger = logging.getLogger(__name__)

# 現在時刻を記録（処理開始時刻として）
start_time = datetime.now()
logger.info(f"データクリーニング処理開始: {start_time}")

# =============================================================================
# 【セクション2: データの読み込みと初期検証】
# =============================================================================

print("\n📖 STEP 1: データ読み込みと基本検証")
print("-" * 50)

try:  # エラーハンドリング開始（ファイル読み込み失敗に備える）
    
    # CSVファイルの読み込み（実際のファイルパスに変更してください）
    file_path = 'sample_data.csv'  # 処理対象ファイルのパス
    
    # encoding='utf-8': 日本語文字化けを防ぐ
    # low_memory=False: 大きなファイルでのデータ型推定エラーを防ぐ
    df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)
    
    logger.info(f"✅ ファイル読み込み成功: {file_path}")
    
except FileNotFoundError:  # ファイルが見つからない場合の処理
    logger.error(f"❌ ファイルが見つかりません: {file_path}")
    print(f"エラー: {file_path} が存在しません。ファイルパスを確認してください。")
    sys.exit(1)  # プログラムを終了（致命的エラーのため）
    
except UnicodeDecodeError:  # 文字エンコーディングエラーの場合
    logger.warning("文字エンコーディングエラー。Shift_JISで再試行します。")
    try:
        # 日本語CSVでよくあるShift_JIS形式で再試行
        df = pd.read_csv(file_path, encoding='shift_jis', low_memory=False)
        logger.info("✅ Shift_JISでの読み込み成功")
    except Exception as e:
        logger.error(f"❌ ファイル読み込み失敗: {e}")
        sys.exit(1)
        
except Exception as e:  # その他の予期しないエラー
    logger.error(f"❌ 予期しないエラー: {e}")
    sys.exit(1)

# =============================================================================
# 【セクション3: データの基本情報分析】
# =============================================================================

print(f"\n🔍 データの基本情報分析")
print("=" * 50)

# データの形状（行数と列数）を取得
rows, columns = df.shape
print(f"📊 データサイズ: {rows:,}行 × {columns}列")  # カンマ区切りで見やすく表示

# メモリ使用量の確認（大きなデータセットでは重要な指標）
memory_usage_mb = df.memory_usage(deep=True).sum() / 1024 / 1024  # バイトをMBに変換
print(f"💾 メモリ使用量: {memory_usage_mb:.2f} MB")

# 各列の基本情報を表示
print(f"\n📋 列の一覧 ({len(df.columns)}個):")
for i, column in enumerate(df.columns, 1):  # enumerate: インデックス付きループ
    # 各列の情報を整理して表示
    dtype = df[column].dtype                    # データ型を取得
    null_count = df[column].isnull().sum()     # 欠損値数をカウント
    null_percentage = (null_count / len(df)) * 100  # 欠損値の割合を計算
    unique_count = df[column].nunique()        # ユニーク値の数を取得
    
    # 情報を見やすい形式で出力
    print(f"  {i:2d}. {column:<25} | 型: {str(dtype):<12} | 欠損: {null_count:>6}個 ({null_percentage:>5.1f}%) | ユニーク: {unique_count:>8}個")

# データ型の分布を確認（どんな種類のデータが多いかを把握）
print(f"\n📈 データ型の分布:")
dtype_counts = df.dtypes.value_counts()  # データ型ごとの列数を集計
for dtype, count in dtype_counts.items():
    print(f"  {str(dtype):<15}: {count:>3}列")

# 欠損値の全体状況を確認
total_missing = df.isnull().sum().sum()  # 全体の欠損値数
total_cells = df.shape[0] * df.shape[1]  # 全セル数
missing_percentage = (total_missing / total_cells) * 100  # 欠損値の全体割合

print(f"\n🚨 欠損値の全体状況:")
print(f"  総欠損値数: {total_missing:,}個")
print(f"  全体に占める割合: {missing_percentage:.2f}%")

# 最初の5行と最後の5行を表示（データの中身を確認）
print(f"\n👀 データの先頭5行:")
print(df.head())  # head(): 最初の5行を表示

print(f"\n👀 データの末尾5行:")
print(df.tail())  # tail(): 最後の5行を表示

# 数値列の基本統計情報
numeric_columns = df.select_dtypes(include=[np.number]).columns  # 数値型の列のみ選択
if len(numeric_columns) > 0:  # 数値列が存在する場合のみ実行
    print(f"\n📊 数値列の基本統計 ({len(numeric_columns)}列):")
    print(df[numeric_columns].describe())  # describe(): 平均、標準偏差、最小値、最大値など
else:
    print(f"\n⚠️  数値列が見つかりませんでした")

# =============================================================================
# 【セクション4: データ品質の詳細分析】
# =============================================================================

print(f"\n🔬 STEP 2: データ品質の詳細分析")
print("-" * 50)

# 重複行の詳細分析
duplicate_count = df.duplicated().sum()  # 重複行の数をカウント
if duplicate_count > 0:  # 重複が存在する場合
    duplicate_percentage = (duplicate_count / len(df)) * 100  # 重複の割合
    print(f"🔴 重複行検出: {duplicate_count:,}行 ({duplicate_percentage:.2f}%)")
    
    # 重複行の詳細を表示
    duplicate_rows = df[df.duplicated(keep=False)]  # keep=False: 重複行を全て表示
    print(f"   重複パターン数: {len(duplicate_rows.drop_duplicates())}種類")
    
else:
    print(f"✅ 重複行: なし")

# 各列の詳細な品質チェック
print(f"\n🔍 各列の品質分析:")
for column in df.columns:  # 全ての列を順次チェック
    
    print(f"\n📝 列: '{column}'")
    print(f"   ├─ データ型: {df[column].dtype}")
    
    # 欠損値の詳細
    null_count = df[column].isnull().sum()
    null_percentage = (null_count / len(df)) * 100
    print(f"   ├─ 欠損値: {null_count:,}個 ({null_percentage:.1f}%)")
    
    # ユニーク値の分析
    unique_count = df[column].nunique()
    unique_percentage = (unique_count / len(df)) * 100
    print(f"   ├─ ユニーク値: {unique_count:,}個 ({unique_percentage:.1f}%)")
    
    # カテゴリ列の場合の追加分析
    if df[column].dtype == 'object':  # 文字列（オブジェクト）型の場合
        # 最頻値を確認
        try:
            mode_value = df[column].mode()[0] if len(df[column].mode()) > 0 else "N/A"
            mode_count = (df[column] == mode_value).sum() if mode_value != "N/A" else 0
            print(f"   ├─ 最頻値: '{mode_value}' ({mode_count}回)")
        except:
            print(f"   ├─ 最頻値: 取得できませんでした")
        
        # 文字列長の分析
        str_lengths = df[column].astype(str).str.len()  # 各値の文字数を計算
        print(f"   ├─ 文字数: 最短{str_lengths.min()} ～ 最長{str_lengths.max()}文字")
        
        # 空文字列や空白のみの値をチェック
        empty_strings = (df[column].astype(str).str.strip() == '').sum()  # 空白を除去して空文字列かチェック
        if empty_strings > 0:
            print(f"   ├─ ⚠️  空文字列: {empty_strings}個")
        
    # 数値列の場合の追加分析
    elif np.issubdtype(df[column].dtype, np.number):  # 数値型の場合
        # 基本統計
        print(f"   ├─ 最小値: {df[column].min()}")
        print(f"   ├─ 最大値: {df[column].max()}")
        print(f"   ├─ 平均値: {df[column].mean():.2f}")
        print(f"   ├─ 中央値: {df[column].median():.2f}")
        
        # 異常値の検出（IQR法）
        Q1 = df[column].quantile(0.25)      # 第1四分位数
        Q3 = df[column].quantile(0.75)      # 第3四分位数
        IQR = Q3 - Q1                       # 四分位範囲
        lower_bound = Q1 - 1.5 * IQR        # 異常値の下限
        upper_bound = Q3 + 1.5 * IQR        # 異常値の上限
        
        # 異常値の数をカウント
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
        outlier_count = len(outliers)
        if outlier_count > 0:
            outlier_percentage = (outlier_count / len(df)) * 100
            print(f"   └─ ⚠️  異常値: {outlier_count}個 ({outlier_percentage:.1f}%) 範囲外: {lower_bound:.1f}～{upper_bound:.1f}")
        else:
            print(f"   └─ ✅ 異常値: なし")
    
    else:
        print(f"   └─ その他の型")

# =============================================================================
# 【セクション5: 列名の標準化とクリーニング】
# =============================================================================

print(f"\n🏷️ STEP 3: 列名の標準化")
print("-" * 50)

# 元の列名をバックアップ（処理前後の比較用）
original_columns = df.columns.tolist()
logger.info(f"元の列名: {original_columns}")

print("列名標準化の処理:")

# 1. 小文字への変換
print("  1️⃣ 小文字変換中...")
df.columns = df.columns.str.lower()
print(f"     結果例: {list(df.columns[:3])}...")  # 最初の3列のみ表示

# 2. 空白をアンダースコアに置換
print("  2️⃣ 空白をアンダースコアに変換中...")
df.columns = df.columns.str.replace(' ', '_')
print(f"     結果例: {list(df.columns[:3])}...")

# 3. 連続する空白を単一のアンダースコアに変換
print("  3️⃣ 連続空白の整理中...")
df.columns = df.columns.str.replace(r'\s+', '_', regex=True)  # 正規表現で複数空白を処理

# 4. 特殊文字の除去（英数字とアンダースコア以外を削除）
print("  4️⃣ 特殊文字除去中...")
df.columns = df.columns.str.replace(r'[^a-zA-Z0-9_]', '', regex=True)

# 5. 連続するアンダースコアを単一に変換
print("  5️⃣ 連続アンダースコアの整理中...")
df.columns = df.columns.str.replace(r'_+', '_', regex=True)

# 6. 先頭・末尾のアンダースコアを除去
print("  6️⃣ 前後アンダースコアの除去中...")
df.columns = df.columns.str.strip('_')

# 7. 空の列名や数字のみの列名への対応
print("  7️⃣ 問題のある列名の修正中...")
for i, column in enumerate(df.columns):
    if column == '' or column.isdigit():  # 空文字列または数字のみの場合
        new_name = f'column_{i+1}'       # デフォルト名を付与
        df.columns.values[i] = new_name   # 列名を変更
        print(f"     ⚠️  問題のある列名を修正: '{original_columns[i]}' → '{new_name}'")

# 8. 重複する列名の処理
print("  8️⃣ 重複列名の処理中...")
column_counts = {}  # 列名の出現回数を記録する辞書
new_columns = []    # 新しい列名のリスト

for column in df.columns:
    if column in column_counts:  # 既に存在する列名の場合
        column_counts[column] += 1
        new_column_name = f"{column}_{column_counts[column]}"  # 連番を付与
        new_columns.append(new_column_name)
        print(f"     ⚠️  重複列名を修正: '{column}' → '{new_column_name}'")
    else:
        column_counts[column] = 1
        new_columns.append(column)

df.columns = new_columns  # 修正した列名を適用

# 変更結果の表示
print(f"\n📊 列名標準化の結果:")
print(f"  処理前列数: {len(original_columns)}")
print(f"  処理後列数: {len(df.columns)}")

changes_made = 0  # 変更があった列数をカウント
print(f"\n🔄 変更された列名:")
for old, new in zip(original_columns, df.columns):
    if old != new:  # 変更があった場合のみ表示
        print(f"  '{old}' → '{new}'")
        changes_made += 1

if changes_made == 0:
    print(f"  変更なし（全ての列名が既に標準形式でした）")
else:
    print(f"  合計 {changes_made} 列の名前を変更しました")

logger.info(f"列名標準化完了: {changes_made}列を変更")

# =============================================================================
# 【セクション6: 重複データの詳細分析と削除】
# =============================================================================

print(f"\n🗑️ STEP 4: 重複データの処理")
print("-" * 50)

# 重複チェック前の状態を記録
initial_rows = len(df)
initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024

print(f"重複チェック開始:")
print(f"  📊 処理前データ: {initial_rows:,}行")
print(f"  💾 処理前メモリ: {initial_memory:.2f} MB")

# 1. 完全重複行の検出
print(f"\n🔍 完全重複行の分析:")
duplicate_mask = df.duplicated(keep=False)  # keep=False: 重複行全てをTrueにする
duplicate_rows = df[duplicate_mask]          # 重複行のみを抽出
unique_duplicate_patterns = df[duplicate_mask].drop_duplicates()  # 重複パターンを取得

if len(duplicate_rows) > 0:  # 重複が存在する場合
    print(f"  🔴 重複行発見: {len(duplicate_rows):,}行")
    print(f"  📋 重複パターン数: {len(unique_duplicate_patterns):,}種類")
    
    # 重複の影響度を分析
    duplicate_percentage = (len(duplicate_rows) / initial_rows) * 100
    print(f"  📈 重複率: {duplicate_percentage:.2f}%")
    
    # 最も多い重複パターンを表示
    if len(unique_duplicate_patterns) > 0:
        print(f"\n  📋 重複パターンの例（最初の3行）:")
        for i, (idx, row) in enumerate(unique_duplicate_patterns.head(3).iterrows()):
            duplicate_count = (df == row).all(axis=1).sum()  # このパターンの重複数
            print(f"    パターン{i+1}: {duplicate_count}回重複")
            # 重要な列だけ表示（全列は多すぎるため）
            display_columns = df.columns[:3] if len(df.columns) >= 3 else df.columns
            for col in display_columns:
                print(f"      {col}: {row[col]}")
            if i < 2:  # 最後以外は区切り線
                print(f"      " + "-" * 30)
else:
    print(f"  ✅ 重複行なし")

# 2. 特定列での重複チェック（ビジネスキーがある場合）
# 例：IDやメールアドレスなど、一意であるべき列
key_columns_candidates = []  # 一意であるべき可能性のある列を探す

for column in df.columns:
    # 列名にid, email, code, numberなどが含まれる場合、キー列候補とする
    if any(keyword in column.lower() for keyword in ['id', 'email', 'code', 'number', 'key']):
        unique_ratio = df[column].nunique() / len(df)  # ユニーク率を計算
        if unique_ratio > 0.95:  # 95%以上がユニークならキー列候補
            key_columns_candidates.append(column)

if key_columns_candidates:
    print(f"\n🔑 ビジネスキー列での重複チェック:")
    for key_column in key_columns_candidates:
        key_duplicates = df[key_column].duplicated().sum()
        if key_duplicates > 0:
            print(f"  ⚠️  '{key_column}': {key_duplicates}件の重複")
            # 重複値の例を表示
            duplicate_values = df[df[key_column].duplicated(keep=False)][key_column].unique()[:3]
            print(f"       重複値例: {list(duplicate_values)}")
        else:
            print(f"  ✅ '{key_column}': 重複なし")

# 3. 重複行の削除実行
print(f"\n🚮 重複行削除の実行:")

if len(duplicate_rows) > 0:
    # 重複行削除前に確認
    print(f"  ❓ {len(duplicate_rows):,}行の重複を削除します...")
    
    # 削除実行（最初の出現を保持、後続を削除）
    df_before_dedup = df.copy()  # 削除前のバックアップ
    df = df.drop_duplicates(keep='first')  # keep='first': 最初の出現を保持
    
    # 削除結果の確認
    final_rows = len(df)
    removed_rows = initial_rows - final_rows
    space_saved = (df_before_dedup.memory_usage(deep=True).sum() - df.memory_usage(deep=True).sum()) / 1024 / 1024
    
    print(f"  ✅ 重複削除完了!")
    print(f"    ├─ 削除前: {initial_rows:,}行")
    print(f"    ├─ 削除後: {final_rows:,}行")
    print(f"    ├─ 削除数: {removed_rows:,}行")
    print(f"    ├─ 削除率: {(removed_rows/initial_rows)*100:.2f}%")
    print(f"    └─ メモリ節約: {space_saved:.2f} MB")
    
    logger.info(f"重複削除: {removed_rows}行削除、{space_saved:.2f}MB節約")
    
else:
    print(f"  ℹ️  削除対象の重複行がないため、スキップします")

# =============================================================================
# 【セクション7: データ型の詳細変換】
# =============================================================================

print(f"\n🔄 STEP 5: データ型の最適化変換")
print("-" * 50)

# 変換前のデータ型を記録
original_dtypes = df.dtypes.copy()
conversion_results = []  # 変換結果を記録するリスト

print(f"データ型変換の開始:")
print(f"  📊 対象列数: {len(df.columns)}列")

# 1. 日付列の自動検出と変換
print(f"\n📅 日付列の検出と変換:")
date_conversion_count = 0  # 変換成功数をカウント

for column in df.columns:
    # 列名に日付を示すキーワードが含まれているかチェック
    date_keywords = ['date', 'time', 'created', 'updated', 'modified', 'birth', 'expire']
    is_potential_date = any(keyword in column.lower() for keyword in date_keywords)
    
    if is_potential_date and df[column].dtype == 'object':  # 文字列型で日付候補の場合
        print(f"  🔍 '{column}' の日付変換を試行中...")
        
        # サンプルデータを確認して日付らしいかチェック
        sample_values = df[column].dropna().head(5).tolist()
        print(f"    サンプル値: {sample_values}")
        
        try:
            # まず数行でテスト変換
            test_series = df[column].head(100).dropna()
            test_converted = pd.to_datetime(test_series, infer_datetime_format=True, errors='coerce')
            
            # 変換成功率をチェック
            success_rate = (test_converted.notna().sum() / len(test_series)) if len(test_series) > 0 else 0
            
            if success_rate >= 0.8:  # 80%以上が正常に変換できる場合のみ実行
                # 全体に適用
                df[column] = pd.to_datetime(df[column], infer_datetime_format=True, errors='coerce')
                
                # 変換結果の確認
                valid_dates = df[column].notna().sum()
                total_values = len(df[column])
                final_success_rate = (valid_dates / total_values) * 100
                
                print(f"    ✅ 変換成功! 有効日付: {valid_dates:,}/{total_values:,} ({final_success_rate:.1f}%)")
                date_conversion_count += 1
                conversion_results.append(f"✅ {column}: object → datetime64")
                
                # 日付範囲の確認
                if valid_dates > 0:
                    min_date = df[column].min()
                    max_date = df[column].max()
                    print(f"    📊 日付範囲: {min_date} ～ {max_date}")
            else:
                print(f"    ❌ 変換成功率が低いためスキップ ({success_rate*100:.1f}%)")
                
        except Exception as e:
            print(f"    ❌ 変換エラー: {str(e)[:50]}...")

print(f"  📈 日付変換結果: {date_conversion_count}列変換完了")

# 2. 数値列の検出と変換
print(f"\n🔢 数値列の検出と変換:")
numeric_conversion_count = 0

# 文字列型の列のみを対象
string_columns = df.select_dtypes(include=['object']).columns

for column in string_columns:
    if column in [col for col in df.columns if df[col].dtype == 'datetime64[ns]']:
        continue  # 既に日付変換された列はスキップ
    
    print(f"  🔍 '{column}' の数値変換を試行中...")
    
    # サンプルデータの確認
    sample_values = df[column].dropna().head(5).tolist()
    print(f"    サンプル値: {sample_values}")
    
    try:
        # データのクリーニング（通貨記号、カンマなどを除去）
        cleaned_series = df[column].astype(str).copy()
        
        # よくある文字列を除去
        print(f"    🧹 文字列クリーニング中...")
        
        # カンマの除去
        cleaned_series = cleaned_series.str.replace(',', '')
        # 通貨記号の除去
        cleaned_series = cleaned_series.str.replace('¥', '').str.replace('$', '').str.replace('€', '')
        # パーセント記号の除去
        cleaned_series = cleaned_series.str.replace('%', '')
        # 前後の空白除去
        cleaned_series = cleaned_series.str.strip()
        # 全角数字を半角に変換（日本語データ対応）
        cleaned_series = cleaned_series.str.translate(str.maketrans('０１２３４５６７８９', '0123456789'))
        
        # 数値変換のテスト
        test_series = cleaned_series.head(100)
        test_converted = pd.to_numeric(test_series, errors='coerce')
        
        # 変換成功率をチェック
        valid_numbers = test_converted.notna().sum()
        success_rate = (valid_numbers / len(test_series)) if len(test_series) > 0 else 0
        
        if success_rate >= 0.8:  # 80%以上が数値に変換可能な場合
            # 全体に適用
            df[column] = pd.to_numeric(cleaned_series, errors='coerce')
            
            # 結果の確認
            final_valid = df[column].notna().sum()
            total_count = len(df[column])
            final_success_rate = (final_valid / total_count) * 100
            
            # データ型の最適化（メモリ節約）
            if df[column].notna().any():  # 有効な数値がある場合
                # 整数かどうかチェック
                if df[column].dropna().apply(lambda x: x == int(x)).all():
                    # 整数の場合、最適なint型を選択
                    min_val = df[column].min()
                    max_val = df[column].max()
                    
                    if min_val >= 0:  # 非負整数の場合
                        if max_val <= 255:
                            df[column] = df[column].astype('uint8')
                            dtype_name = 'uint8'
                        elif max_val <= 65535:
                            df[column] = df[column].astype('uint16')
                            dtype_name = 'uint16'
                        else:
                            df[column] = df[column].astype('uint32')
                            dtype_name = 'uint32'
                    else:  # 符号付き整数の場合
                        if min_val >= -128 and max_val <= 127:
                            df[column] = df[column].astype('int8')
                            dtype_name = 'int8'
                        elif min_val >= -32768 and max_val <= 32767:
                            df[column] = df[column].astype('int16')
                            dtype_name = 'int16'
                        else:
                            df[column] = df[column].astype('int32')
                            dtype_name = 'int32'
                else:
                    # 浮動小数点数の場合
                    df[column] = df[column].astype('float32')  # メモリ節約でfloat32使用
                    dtype_name = 'float32'
                
                print(f"    ✅ 変換成功! 有効数値: {final_valid:,}/{total_count:,} ({final_success_rate:.1f}%) → {dtype_name}")
                numeric_conversion_count += 1
                conversion_results.append(f"✅ {column}: object → {dtype_name}")
                
                # 数値範囲の確認
                if final_valid > 0:
                    min_val = df[column].min()
                    max_val = df[column].max()
                    mean_val = df[column].mean()
                    print(f"    📊 数値範囲: {min_val:.2f} ～ {max_val:.2f} (平均: {mean_val:.2f})")
            else:
                print(f"    ❌ 有効な数値がありません")
        else:
            print(f"    ❌ 変換成功率が低いためスキップ ({success_rate*100:.1f}%)")
            
    except Exception as e:
        print(f"    ❌ 変換エラー: {str(e)[:50]}...")

print(f"  📈 数値変換結果: {numeric_conversion_count}列変換完了")

# 3. カテゴリ型への最適化
print(f"\n📂 カテゴリ型への最適化:")
category_conversion_count = 0

# 残りの文字列列をカテゴリ型に変換（メモリ節約）
remaining_string_columns = df.select_dtypes(include=['object']).columns

for column in remaining_string_columns:
    unique_count = df[column].nunique()
    total_count = len(df[column])
    unique_ratio = unique_count / total_count if total_count > 0 else 0
    
    # ユニーク率が50%未満の場合、カテゴリ型に変換（メモリ節約効果大）
    if unique_ratio < 0.5 and unique_count < 1000:  # 1000種類未満のカテゴリ
        memory_before = df[column].memory_usage(deep=True) / 1024 / 1024
        
        df[column] = df[column].astype('category')
        
        memory_after = df[column].memory_usage(deep=True) / 1024 / 1024
        memory_saved = memory_before - memory_after
        
        print(f"  ✅ '{column}': カテゴリ化完了")
        print(f"    📊 カテゴリ数: {unique_count:,}種類 ({unique_ratio*100:.1f}%)")
        print(f"    💾 メモリ節約: {memory_saved:.2f} MB")
        
        category_conversion_count += 1
        conversion_results.append(f"✅ {column}: object → category")

print(f"  📈 カテゴリ変換結果: {category_conversion_count}列変換完了")

# 変換結果のサマリー
print(f"\n📊 データ型変換のサマリー:")
total_conversions = date_conversion_count + numeric_conversion_count + category_conversion_count
print(f"  🎯 総変換列数: {total_conversions}列")
print(f"    ├─ 日付変換: {date_conversion_count}列")
print(f"    ├─ 数値変換: {numeric_conversion_count}列")
print(f"    └─ カテゴリ変換: {category_conversion_count}列")

# メモリ使用量の変化
current_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
memory_change = initial_memory - current_memory
print(f"  💾 メモリ使用量変化: {initial_memory:.2f} MB → {current_memory:.2f} MB")
if memory_change > 0:
    print(f"    ✅ {memory_change:.2f} MB削減 ({(memory_change/initial_memory)*100:.1f}%減)")
else:
    print(f"    📊 {abs(memory_change):.2f} MB増加")

# 変更された列の詳細リスト
if conversion_results:
    print(f"\n🔄 変換された列の詳細:")
    for result in conversion_results:
        print(f"    {result}")

logger.info(f"データ型変換完了: {total_conversions}列変換、{memory_change:.2f}MB節約")

# =============================================================================
# 【セクション8: 欠損値の高度な処理】
# =============================================================================

print(f"\n🩹 STEP 6: 欠損値の戦略的処理")
print("-" * 50)

# 欠損値処理前の状態記録
missing_before_dict = df.isnull().sum().to_dict()  # 列ごとの欠損値数
total_missing_before = df.isnull().sum().sum()     # 全体の欠損値数

print(f"欠損値処理の開始:")
print(f"  📊 処理前総欠損値: {total_missing_before:,}個")

if total_missing_before == 0:
    print(f"  ✅ 欠損値がないため、このステップをスキップします")
else:
    print(f"  🎯 戦略的欠損値補完を実行します")

    # 欠損値パターンの分析
    print(f"\n🔍 欠損値パターンの分析:")
    
    # 欠損値が多い列の特定
    high_missing_threshold = 0.5  # 50%以上欠損している列
    high_missing_columns = []
    
    for column in df.columns:
        missing_count = df[column].isnull().sum()
        missing_rate = missing_count / len(df)
        
        if missing_rate >= high_missing_threshold:
            high_missing_columns.append((column, missing_count, missing_rate))
            print(f"    ⚠️  高欠損列: '{column}' - {missing_count:,}個 ({missing_rate*100:.1f}%)")
    
    if high_missing_columns:
        print(f"    💡 推奨: 高欠損列は削除を検討してください")
    
    # 1. 数値列の欠損値処理
    print(f"\n🔢 数値列の欠損値処理:")
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    numeric_missing_handled = 0
    
    for column in numeric_columns:
        missing_count = df[column].isnull().sum()
        
        if missing_count > 0:
            missing_rate = missing_count / len(df)
            print(f"  🔍 '{column}': {missing_count:,}個欠損 ({missing_rate*100:.1f}%)")
            
            if missing_rate < high_missing_threshold:  # 欠損率が50%未満の場合のみ補完
                # 補完方法の決定（データの分布に基づく）
                try:
                    # 外れ値の影響を確認
                    Q1 = df[column].quantile(0.25)
                    Q3 = df[column].quantile(0.75)
                    IQR = Q3 - Q1
                    
                    # 外れ値の除去後の統計値を計算
                    clean_data = df[column][(df[column] >= Q1 - 1.5*IQR) & (df[column] <= Q3 + 1.5*IQR)]
                    
                    mean_val = clean_data.mean()
                    median_val = clean_data.median()
                    
                    # 平均と中央値の差で補完方法を決定
                    if abs(mean_val - median_val) / median_val < 0.1:  # 差が10%未満
                        # 正規分布に近い場合は平均値で補完
                        fill_value = mean_val
                        method = "平均値"
                    else:
                        # 偏った分布の場合は中央値で補完
                        fill_value = median_val
                        method = "中央値"
                    
                    # 補完実行
                    df[column].fillna(fill_value, inplace=True)
                    
                    print(f"    ✅ {method}({fill_value:.2f})で補完完了")
                    numeric_missing_handled += 1
                    
                except Exception as e:
                    # エラーの場合は0で補完
                    df[column].fillna(0, inplace=True)
                    print(f"    ⚠️  エラーのため0で補完: {str(e)[:30]}...")
                    numeric_missing_handled += 1
            else:
                print(f"    ❌ 欠損率が高いため補完をスキップ")
    
    print(f"  📈 数値列処理結果: {numeric_missing_handled}列処理完了")
    
    # 2. カテゴリ列の欠損値処理
    print(f"\n📂 カテゴリ列の欠損値処理:")
    categorical_columns = df.select_dtypes(include=['object', 'category']).columns
    categorical_missing_handled = 0
    
    for column in categorical_columns:
        missing_count = df[column].isnull().sum()
        
        if missing_count > 0:
            missing_rate = missing_count / len(df)
            print(f"  🔍 '{column}': {missing_count:,}個欠損 ({missing_rate*100:.1f}%)")
            
            if missing_rate < high_missing_threshold:
                try:
                    # 最頻値を取得
                    mode_values = df[column].mode()
                    
                    if len(mode_values) > 0 and pd.notna(mode_values[0]):
                        # 最頻値で補完
                        fill_value = mode_values[0]
                        fill_count = (df[column] == fill_value).sum()
                        total_non_null = df[column].notna().sum()
                        frequency_rate = fill_count / total_non_null if total_non_null > 0 else 0
                        
                        df[column].fillna(fill_value, inplace=True)
                        
                        print(f"    ✅ 最頻値('{fill_value}')で補完完了")
                        print(f"      📊 最頻値の出現率: {frequency_rate*100:.1f}%")
                        categorical_missing_handled += 1
                        
                    else:
                        # 最頻値がない場合は'Unknown'で補完
                        df[column].fillna('Unknown', inplace=True)
                        print(f"    ✅ 'Unknown'で補完完了")
                        categorical_missing_handled += 1
                        
                except Exception as e:
                    # エラーの場合は'Missing'で補完
                    df[column].fillna('Missing', inplace=True)
                    print(f"    ⚠️  エラーのため'Missing'で補完: {str(e)[:30]}...")
                    categorical_missing_handled += 1
            else:
                print(f"    ❌ 欠損率が高いため補完をスキップ")
    
    print(f"  📈 カテゴリ列処理結果: {categorical_missing_handled}列処理完了")
    
    # 3. 日付列の欠損値処理
    print(f"\n📅 日付列の欠損値処理:")
    datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns
    datetime_missing_handled = 0
    
    for column in datetime_columns:
        missing_count = df[column].isnull().sum()
        
        if missing_count > 0:
            missing_rate = missing_count / len(df)
            print(f"  🔍 '{column}': {missing_count:,}個欠損 ({missing_rate*100:.1f}%)")
            
            if missing_rate < high_missing_threshold:
                try:
                    # 前方補完と後方補完を組み合わせ
                    original_missing = df[column].isnull().sum()
                    
                    # 前方補完（前の値で埋める）
                    df[column].fillna(method='ffill', inplace=True)
                    after_ffill_missing = df[column].isnull().sum()
                    
                    # 後方補完（後の値で埋める）
                    df[column].fillna(method='bfill', inplace=True)
                    final_missing = df[column].isnull().sum()
                    
                    filled_count = original_missing - final_missing
                    
                    print(f"    ✅ 前後補完で{filled_count:,}個補完完了")
                    if final_missing > 0:
                        print(f"      ⚠️  {final_missing}個は補完できませんでした")
                    
                    datetime_missing_handled += 1
                    
                except Exception as e:
                    print(f"    ❌ 補完エラー: {str(e)[:30]}...")
            else:
                print(f"    ❌ 欠損率が高いため補完をスキップ")
    
    print(f"  📈 日付列処理結果: {datetime_missing_handled}列処理完了")
    
    # 処理結果のサマリー
    missing_after_dict = df.isnull().sum().to_dict()
    total_missing_after = df.isnull().sum().sum()
    total_filled = total_missing_before - total_missing_after
    
    print(f"\n📊 欠損値処理のサマリー:")
    print(f"  🎯 総処理列数: {numeric_missing_handled + categorical_missing_handled + datetime_missing_handled}列")
    print(f"    ├─ 数値列: {numeric_missing_handled}列")
    print(f"    ├─ カテゴリ列: {categorical_missing_handled}列")
    print(f"    └─ 日付列: {datetime_missing_handled}列")
    print(f"  📈 欠損値の変化:")
    print(f"    ├─ 処理前: {total_missing_before:,}個")
    print(f"    ├─ 処理後: {total_missing_after:,}個")
    print(f"    ├─ 補完数: {total_filled:,}個")
    print(f"    └─ 補完率: {(total_filled/total_missing_before)*100:.1f}%" if total_missing_before > 0 else "    └─ 補完率: N/A")
    
    # 残存欠損値がある列の報告
    if total_missing_after > 0:
        print(f"\n⚠️  残存欠損値がある列:")
        for column in df.columns:
            remaining_missing = df[column].isnull().sum()
            if remaining_missing > 0:
                missing_rate = remaining_missing / len(df)
                print(f"    '{column}': {remaining_missing:,}個 ({missing_rate*100:.1f}%)")
    else:
        print(f"\n🎉 全ての欠損値が処理されました！")

logger.info(f"欠損値処理完了: {total_filled}個補完、{total_missing_after}個残存")

# =============================================================================
# 【セクション9: 異常値の検出と処理】
# =============================================================================

print(f"\n🚨 STEP 7: 異常値の検出と処理")
print("-" * 50)

# 数値列のみを対象とする
numeric_columns = df.select_dtypes(include=[np.number]).columns

if len(numeric_columns) == 0:
    print(f"  ℹ️  数値列がないため、異常値処理をスキップします")
else:
    print(f"異常値検出の開始:")
    print(f"  🎯 対象列数: {len(numeric_columns)}列")
    
    outlier_summary = []  # 異常値処理結果を記録
    
    for column in numeric_columns:
        print(f"\n🔍 '{column}' の異常値分析:")
        
        # 基本統計の確認
        valid_data = df[column].dropna()  # 欠損値を除外
        
        if len(valid_data) == 0:
            print(f"    ❌ 有効なデータがありません")
            continue
        
        # 基本統計量の表示
        print(f"    📊 基本統計:")
        print(f"      ├─ 件数: {len(valid_data):,}")
        print(f"      ├─ 平均: {valid_data.mean():.2f}")
        print(f"      ├─ 中央値: {valid_data.median():.2f}")
        print(f"      ├─ 標準偏差: {valid_data.std():.2f}")
        print(f"      ├─ 最小値: {valid_data.min():.2f}")
        print(f"      └─ 最大値: {valid_data.max():.2f}")
        
        # IQR法による異常値検出
        Q1 = valid_data.quantile(0.25)      # 第1四分位数
        Q3 = valid_data.quantile(0.75)      # 第3四分位数
        IQR = Q3 - Q1                       # 四分位範囲
        
        # 異常値の閾値計算
        lower_bound = Q1 - 1.5 * IQR        # 下限
        upper_bound = Q3 + 1.5 * IQR        # 上限
        
        print(f"    🎯 IQR異常値検出:")
        print(f"      ├─ Q1 (25%): {Q1:.2f}")
        print(f"      ├─ Q3 (75%): {Q3:.2f}")
        print(f"      ├─ IQR: {IQR:.2f}")
        print(f"      ├─ 下限閾値: {lower_bound:.2f}")
        print(f"      └─ 上限閾値: {upper_bound:.2f}")
        
        # 異常値の特定
        outlier_mask = (df[column] < lower_bound) | (df[column] > upper_bound)
        outliers = df[outlier_mask][column].dropna()
        outlier_count = len(outliers)
        
        if outlier_count > 0:
            outlier_percentage = (outlier_count / len(valid_data)) * 100
            print(f"    🔴 異常値検出: {outlier_count:,}個 ({outlier_percentage:.2f}%)")
            
            # 異常値の詳細分析
            lower_outliers = df[df[column] < lower_bound][column].dropna()
            upper_outliers = df[df[column] > upper_bound][column].dropna()
            
            print(f"      ├─ 下限超過: {len(lower_outliers):,}個")
            if len(lower_outliers) > 0:
                print(f"      │   └─ 最小値: {lower_outliers.min():.2f}")
            
            print(f"      └─ 上限超過: {len(upper_outliers):,}個")
            if len(upper_outliers) > 0:
                print(f"          └─ 最大値: {upper_outliers.max():.2f}")
            
            # 異常値の例を表示（最大5個）
            if outlier_count <= 10:
                print(f"      📋 異常値一覧: {sorted(outliers.tolist())}")
            else:
                sample_outliers = sorted(outliers.tolist())[:5]
                print(f"      📋 異常値例(5個): {sample_outliers}...")
            
            # 異常値処理の決定
            if outlier_percentage <= 5.0:  # 5%以下の場合は処理を実行
                print(f"    🔧 異常値処理を実行します...")
                
                # 処理方法の選択
                if IQR > 0:  # IQRが0でない場合（データにばらつきがある）
                    # クリッピング（閾値で制限）
                    original_values = df[column].copy()
                    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
                    
                    # 処理結果の確認
                    clipped_count = (original_values != df[column]).sum()
                    print(f"      ✅ クリッピング完了: {clipped_count:,}値を修正")
                    
                    outlier_summary.append({
                        'column': column,
                        'outlier_count': outlier_count,
                        'outlier_percentage': outlier_percentage,
                        'method': 'クリッピング',
                        'processed_count': clipped_count
                    })
                else:
                    print(f"      ⚠️  データにばらつきがないため処理をスキップ")
            else:
                print(f"    ⚠️  異常値が多すぎるため処理をスキップ (>{outlier_percentage:.1f}%)")
                print(f"       💡 手動での確認をお勧めします")
                
                outlier_summary.append({
                    'column': column,
                    'outlier_count': outlier_count,
                    'outlier_percentage': outlier_percentage,
                    'method': 'スキップ',
                    'processed_count': 0
                })
        else:
            print(f"    ✅ 異常値なし")
            outlier_summary.append({
                'column': column,
                'outlier_count': 0,
                'outlier_percentage': 0.0,
                'method': '処理不要',
                'processed_count': 0
            })
    
    # 異常値処理のサマリー
    print(f"\n📊 異常値処理のサマリー:")
    total_outliers_found = sum([item['outlier_count'] for item in outlier_summary])
    total_outliers_processed = sum([item['processed_count'] for item in outlier_summary])
    processed_columns = len([item for item in outlier_summary if item['processed_count'] > 0])
    
    print(f"  🎯 処理結果:")
    print(f"    ├─ 検査列数: {len(numeric_columns)}列")
    print(f"    ├─ 異常値発見数: {total_outliers_found:,}個")
    print(f"    ├─ 処理済み異常値: {total_outliers_processed:,}個")
    print(f"    └─ 処理列数: {processed_columns}列")
    
    # 列別の詳細結果
    if outlier_summary:
        print(f"\n📋 列別処理結果:")
        for item in outlier_summary:
            if item['outlier_count'] > 0:
                print(f"    '{item['column']}': {item['outlier_count']:,}個異常値 → {item['method']}")

logger.info(f"異常値処理完了: {total_outliers_processed}個処理")

# =============================================================================
# 【セクション10: Tableau最適化処理】
# =============================================================================

print(f"\n⚡ STEP 8: Tableau用データ最適化")
print("-" * 50)

tableau_optimizations = []  # 最適化内容を記録

print(f"Tableau最適化の開始:")

# 1. 日付列からの時間軸列生成
print(f"\n📅 日付列から時間軸列の生成:")
datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns
date_columns_added = 0

for column in datetime_columns:
    print(f"  🔍 '{column}' から時間軸列を生成中...")
    
    # ベース名の生成（_date, _timeを除去）
    base_name = column.replace('_date', '').replace('_time', '').replace('date_', '').replace('time_', '')
    if base_name == column:  # 変更がない場合は短縮形を作成
        base_name = column[:10] if len(column) > 10 else column
    
    try:
        # 年列
        year_col = f'{base_name}_year'
        df[year_col] = df[column].dt.year
        print(f"    ✅ {year_col}")
        date_columns_added += 1
        
        # 月列（数値）
        month_col = f'{base_name}_month'
        df[month_col] = df[column].dt.month
        print(f"    ✅ {month_col}")
        date_columns_added += 1
        
        # 月名列（文字列）
        month_name_col = f'{base_name}_month_name'
        df[month_name_col] = df[column].dt.month_name()
        print(f"    ✅ {month_name_col}")
        date_columns_added += 1
        
        # 四半期列
        quarter_col = f'{base_name}_quarter'
        df[quarter_col] = df[column].dt.quarter
        print(f"    ✅ {quarter_col}")
        date_columns_added += 1
        
        # 曜日列（数値: 月曜=0）
        weekday_col = f'{base_name}_weekday'
        df[weekday_col] = df[column].dt.weekday
        print(f"    ✅ {weekday_col}")
        date_columns_added += 1
        
        # 曜日名列（文字列）
        weekday_name_col = f'{base_name}_weekday_name'
        df[weekday_name_col] = df[column].dt.day_name()
        print(f"    ✅ {weekday_name_col}")
        date_columns_added += 1
        
        # 日列
        day_col = f'{base_name}_day'
        df[day_col] = df[column].dt.day
        print(f"    ✅ {day_col}")
        date_columns_added += 1
        
        # 年月列（文字列: YYYY-MM形式）
        year_month_col = f'{base_name}_year_month'
        df[year_month_col] = df[column].dt.strftime('%Y-%m')
        print(f"    ✅ {year_month_col}")
        date_columns_added += 1
        
        # 会計年度列（4月始まり）
        fiscal_year_col = f'{base_name}_fiscal_year'
        df[fiscal_year_col] = df[column].apply(
            lambda x: x.year if x.month >= 4 else x.year - 1 if pd.notna(x) else np.nan
        )
        print(f"    ✅ {fiscal_year_col} (4月始まり)")
        date_columns_added += 1
        
        tableau_optimizations.append(f"日付展開: {column} → 9個の時間軸列")
        
    except Exception as e:
        print(f"    ❌ エラー: {str(e)[:50]}...")

print(f"  📈 時間軸列生成結果: {date_columns_added}列追加")

# 2. カテゴリ型への最適化（メモリ効率化）
print(f"\n📂 カテゴリ型最適化:")
category_optimizations = 0
memory_saved_total = 0

string_columns = df.select_dtypes(include=['object']).columns

for column in string_columns:
    unique_count = df[column].nunique()
    total_count = len(df[column])
    unique_ratio = unique_count / total_count if total_count > 0 else 0
    
    # カテゴリ化の条件：
    # 1. ユニーク率が50%未満
    # 2. ユニーク値が1000個未満
    # 3. データが存在する
    if unique_ratio < 0.5 and unique_count < 1000 and total_count > 0:
        # メモリ使用量の測定
        memory_before = df[column].memory_usage(deep=True) / 1024 / 1024
        
        # カテゴリ型に変換
        df[column] = df[column].astype('category')
        
        memory_after = df[column].memory_usage(deep=True) / 1024 / 1024
        memory_saved = memory_before - memory_after
        memory_saved_total += memory_saved
        
        print(f"  ✅ '{column}': カテゴリ化完了")
        print(f"    📊 {unique_count:,}カテゴリ ({unique_ratio*100:.1f}%)")
        print(f"    💾 メモリ節約: {memory_saved:.2f} MB")
        
        category_optimizations += 1
        tableau_optimizations.append(f"カテゴリ化: {column} ({memory_saved:.1f}MB節約)")

print(f"  📈 カテゴリ最適化結果: {category_optimizations}列、{memory_saved_total:.2f}MB節約")

# 3. 数値列の精度最適化
print(f"\n🔢 数値列の精度最適化:")
numeric_optimizations = 0
numeric_memory_saved = 0

numeric_columns = df.select_dtypes(include=[np.number]).columns

for column in numeric_columns:
    if df[column].notna().any():  # 有効な値がある場合のみ処理
        memory_before = df[column].memory_usage(deep=True) / 1024 / 1024
        original_dtype = df[column].dtype
        
        try:
            # 整数列の最適化
            if df[column].dropna().apply(lambda x: x == int(x)).all():
                min_val = df[column].min()
                max_val = df[column].max()
                
                # 最適な整数型を選択
                if min_val >= 0:  # 非負整数
                    if max_val <= 255:
                        optimal_dtype = 'uint8'
                    elif max_val <= 65535:
                        optimal_dtype = 'uint16'
                    elif max_val <= 4294967295:
                        optimal_dtype = 'uint32'
                    else:
                        optimal_dtype = 'uint64'
                else:  # 符号付き整数
                    if min_val >= -128 and max_val <= 127:
                        optimal_dtype = 'int8'
                    elif min_val >= -32768 and max_val <= 32767:
                        optimal_dtype = 'int16'
                    elif min_val >= -2147483648 and max_val <= 2147483647:
                        optimal_dtype = 'int32'
                    else:
                        optimal_dtype = 'int64'
                
                # 型変換実行
                if optimal_dtype != str(original_dtype):
                    df[column] = df[column].astype(optimal_dtype)
                    memory_after = df[column].memory_usage(deep=True) / 1024 / 1024
                    memory_saved = memory_before - memory_after
                    
                    if memory_saved > 0:
                        print(f"  ✅ '{column}': {original_dtype} → {optimal_dtype}")
                        print(f"    💾 メモリ節約: {memory_saved:.3f} MB")
                        numeric_optimizations += 1
                        numeric_memory_saved += memory_saved
                        tableau_optimizations.append(f"数値最適化: {column} ({memory_saved:.2f}MB節約)")
            
            # 浮動小数点数の最適化
            elif str(original_dtype) == 'float64':
                # float32で精度が保たれるかチェック
                test_float32 = df[column].astype('float32')
                if df[column].equals(test_float32.astype('float64')):
                    df[column] = test_float32
                    memory_after = df[column].memory_usage(deep=True) / 1024 / 1024
                    memory_saved = memory_before - memory_after
                    
                    print(f"  ✅ '{column}': float64 → float32")
                    print(f"    💾 メモリ節約: {memory_saved:.3f} MB")
                    numeric_optimizations += 1
                    numeric_memory_saved += memory_saved
                    tableau_optimizations.append(f"浮動小数点最適化: {column} ({memory_saved:.2f}MB節約)")
        
        except Exception as e:
            print(f"  ⚠️  '{column}': 最適化スキップ - {str(e)[:30]}...")

print(f"  📈 数値最適化結果: {numeric_optimizations}列、{numeric_memory_saved:.3f}MB節約")

# 4. データの並び替え（Tableau表示順最適化）
print(f"\n🔄 データ並び替え最適化:")

# 日付列がある場合、最新日付順に並び替え
datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns
if len(datetime_columns) > 0:
    # 最初の日付列で降順ソート
    primary_date_col = datetime_columns[0]
    df = df.sort_values(by=primary_date_col, ascending=False, na_position='last')
    print(f"  ✅ '{primary_date_col}'で降順ソート（最新データが上位）")
    tableau_optimizations.append(f"並び替え: {primary_date_col}で降順")
else:
    print(f"  ℹ️  日付列がないため、並び替えをスキップ")

# 5. インデックスのリセット
print(f"\n🔄 インデックスのリセット:")
df = df.reset_index(drop=True)  # インデックスを0から連番に振り直し
print(f"  ✅ インデックスを0から連番にリセット")

# 最適化結果のサマリー
print(f"\n📊 Tableau最適化のサマリー:")
total_memory_saved = memory_saved_total + numeric_memory_saved
total_columns_added = date_columns_added
total_optimizations = category_optimizations + numeric_optimizations

print(f"  🎯 最適化結果:")
print(f"    ├─ 新規追加列: {total_columns_added}列")
print(f"    ├─ 最適化列数: {total_optimizations}列")
print(f"    ├─ 総メモリ節約: {total_memory_saved:.2f} MB")
print(f"    └─ 最終列数: {len(df.columns)}列")

# 最終データ形状
final_shape = df.shape
print(f"  📏 最終データサイズ: {final_shape[0]:,}行 × {final_shape[1]}列")

# 実行された最適化の詳細
if tableau_optimizations:
    print(f"\n📋 実行された最適化:")
    for optimization in tableau_optimizations:
        print(f"    • {optimization}")

logger.info(f"Tableau最適化完了: {total_columns_added}列追加、{total_memory_saved:.2f}MB節約")

# =============================================================================
# 【セクション11: 最終検証とエクスポート】
# =============================================================================

print(f"\n🎉 STEP 9: 最終検証とデータエクスポート")
print("-" * 50)

# 処理終了時刻の記録
end_time = datetime.now()
processing_time = end_time - start_time

print(f"データクリーニング処理の完了:")
print(f"  ⏱️  処理時間: {processing_time}")
print(f"  📅 開始時刻: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"  📅 終了時刻: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")

# 最終データの品質確認
print(f"\n🔍 最終データ品質チェック:")

# 1. 基本統計
final_rows, final_columns = df.shape
final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
final_missing = df.isnull().sum().sum()

print(f"  📊 データ概要:")
print(f"    ├─ 行数: {final_rows:,}")
print(f"    ├─ 列数: {final_columns}")
print(f"    ├─ メモリ使用量: {final_memory:.2f} MB")
print(f"    └─ 残存欠損値: {final_missing:,}個")

# 2. データ型の分布
print(f"\n  📈 データ型分布:")
final_dtype_counts = df.dtypes.value_counts()
for dtype, count in final_dtype_counts.items():
    print(f"    {str(dtype):<15}: {count:>3}列")

# 3. メモリ効率の確認
if initial_memory > 0:  # 初期メモリが記録されている場合
    memory_change = initial_memory - final_memory
    memory_efficiency = (memory_change / initial_memory) * 100 if initial_memory > 0 else 0
    
    print(f"\n  💾 メモリ効率:")
    print(f"    ├─ 初期: {initial_memory:.2f} MB")
    print(f"    ├─ 最終: {final_memory:.2f} MB")
    if memory_change > 0:
        print(f"    ├─ 削減: {memory_change:.2f} MB ({memory_efficiency:.1f}%)")
        print(f"    └─ 効率: ✅ 最適化成功")
    else:
        print(f"    ├─ 増加: {abs(memory_change):.2f} MB")
        print(f"    └─ 効率: ⚠️  メモリ増加（機能追加による）")

# 4. Tableau適性チェック
print(f"\n  🎯 Tableau適性チェック:")

tableau_readiness_score = 0
max_score = 6

# 日付列の存在
datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns
if len(datetime_columns) > 0:
    print(f"    ✅ 日付列: {len(datetime_columns)}列存在")
    tableau_readiness_score += 1
else:
    print(f"    ⚠️  日付列: なし")

# 数値列の存在
numeric_columns = df.select_dtypes(include=[np.number]).columns
if len(numeric_columns) > 0:
    print(f"    ✅ 数値列: {len(numeric_columns)}列存在")
    tableau_readiness_score += 1
else:
    print(f"    ⚠️  数値列: なし")

# カテゴリ列の存在
categorical_columns = df.select_dtypes(include=['object', 'category']).columns
if len(categorical_columns) > 0:
    print(f"    ✅ カテゴリ列: {len(categorical_columns)}列存在")
    tableau_readiness_score += 1
else:
    print(f"    ⚠️  カテゴリ列: なし")

# 欠損値の状況
if final_missing == 0:
    print(f"    ✅ 欠損値: なし")
    tableau_readiness_score += 1
elif final_missing < final_rows * final_columns * 0.05:  # 5%未満
    print(f"    ✅ 欠損値: 少量 ({final_missing:,}個)")
    tableau_readiness_score += 1
else:
    print(f"    ⚠️  欠損値: 多数 ({final_missing:,}個)")

# 列名の標準化
non_standard_columns = []
for col in df.columns:
    if not col.islower() or ' ' in col or any(char in col for char in '!@#$%^&*()+=[]{}|;:,<>?'):
        non_standard_columns.append(col)

if len(non_standard_columns) == 0:
    print(f"    ✅ 列名: 標準化済み")
    tableau_readiness_score += 1
else:
    print(f"    ⚠️  列名: {len(non_standard_columns)}列が非標準")

# データサイズ
if final_rows > 0 and final_columns > 0:
    print(f"    ✅ データサイズ: 適切")
    tableau_readiness_score += 1
else:
    print(f"    ❌ データサイズ: 問題あり")

# スコアの表示
readiness_percentage = (tableau_readiness_score / max_score) * 100
print(f"\n    🏆 Tableau適性スコア: {tableau_readiness_score}/{max_score} ({readiness_percentage:.0f}%)")

if readiness_percentage >= 80:
    print(f"    🎉 Tableau導入準備完了！")
elif readiness_percentage >= 60:
    print(f"    ✅ 基本的なTableau利用が可能")
elif readiness_percentage >= 40:
    print(f"    ⚠️  追加の調整が推奨されます")
else:
    print(f"    ❌ 大幅な修正が必要です")

# 5. エクスポート処理
print(f"\n💾 データエクスポート:")

# 出力ファイル名の生成
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
base_filename = 'tableau_ready_data'

# CSV形式でエクスポート（最も一般的）
csv_filename = f'{base_filename}_{timestamp}.csv'
try:
    df.to_csv(csv_filename, index=False, encoding='utf-8')
    csv_size = os.path.getsize(csv_filename) / 1024 / 1024  # MB
    print(f"  ✅ CSV出力完了: {csv_filename}")
    print(f"    📁 ファイルサイズ: {csv_size:.2f} MB")
except Exception as e:
    print(f"  ❌ CSV出力エラー: {str(e)[:50]}...")

# Excel形式でエクスポート（プレビュー用）
if final_rows <= 1000000:  # 100万行以下の場合のみ（Excelの制限考慮）
    excel_filename = f'{base_filename}_{timestamp}.xlsx'
    try:
        df.to_excel(excel_filename, index=False, engine='openpyxl')
        excel_size = os.path.getsize(excel_filename) / 1024 / 1024
        print(f"  ✅ Excel出力完了: {excel_filename}")
        print(f"    📁 ファイルサイズ: {excel_size:.2f} MB")
    except Exception as e:
        print(f"  ⚠️  Excel出力スキップ: {str(e)[:50]}...")
else:
    print(f"  ℹ️  Excel出力スキップ: データサイズが大きすぎます ({final_rows:,}行)")

# Parquet形式でエクスポート（大容量データ推奨）
if final_rows > 100000:  # 10万行以上の場合
    parquet_filename = f'{base_filename}_{timestamp}.parquet'
    try:
        df.to_parquet(parquet_filename, index=False, compression='snappy')
        parquet_size = os.path.getsize(parquet_filename) / 1024 / 1024
        print(f"  ✅ Parquet出力完了: {parquet_filename}")
        print(f"    📁 ファイルサイズ: {parquet_size:.2f} MB")
        print(f"    💡 大容量データのためParquet形式を推奨")
    except Exception as e:
        print(f"  ⚠️  Parquet出力エラー: {str(e)[:50]}...")

# データ辞書の生成（列の詳細情報）
print(f"\n📚 データ辞書の生成:")
dictionary_filename = f'data_dictionary_{timestamp}.txt'

try:
    with open(dictionary_filename, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("データ辞書 (Data Dictionary)\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"生成日時: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"データサイズ: {final_rows:,}行 × {final_columns}列\n")
        f.write(f"メモリ使用量: {final_memory:.2f} MB\n")
        f.write(f"欠損値: {final_missing:,}個\n\n")
        
        f.write("列の詳細情報:\n")
        f.write("-" * 80 + "\n")
        
        for i, column in enumerate(df.columns, 1):
            f.write(f"\n{i:3d}. {column}\n")
            f.write(f"     データ型: {df[column].dtype}\n")
            f.write(f"     欠損値: {df[column].isnull().sum():,}個\n")
            f.write(f"     ユニーク値: {df[column].nunique():,}個\n")
            
            if df[column].dtype in ['object', 'category']:
                # カテゴリ列の場合
                if df[column].nunique() <= 10:
                    unique_values = df[column].unique()
                    f.write(f"     値一覧: {list(unique_values)}\n")
                else:
                    top_values = df[column].value_counts().head(5)
                    f.write(f"     上位5値: {dict(top_values)}\n")
            elif np.issubdtype(df[column].dtype, np.number):
                # 数値列の場合
                f.write(f"     最小値: {df[column].min()}\n")
                f.write(f"     最大値: {df[column].max()}\n")
                f.write(f"     平均値: {df[column].mean():.2f}\n")
            elif df[column].dtype == 'datetime64[ns]':
                # 日付列の場合
                f.write(f"     最早日: {df[column].min()}\n")
                f.write(f"     最遅日: {df[column].max()}\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("処理完了\n")
    
    print(f"  ✅ データ辞書生成完了: {dictionary_filename}")
    
except Exception as e:
    print(f"  ⚠️  データ辞書生成エラー: {str(e)[:50]}...")

# 最終サマリーレポート
print(f"\n" + "=" * 60)
print(f"🎊 データクリーニング完了サマリー")
print(f"=" * 60)

print(f"📊 処理結果:")
print(f"  ├─ 処理時間: {processing_time}")
print(f"  ├─ 初期データ: {initial_rows:,}行 × {len(original_columns)}列")
print(f"  ├─ 最終データ: {final_rows:,}行 × {final_columns}列")
print(f"  ├─ メモリ効率: {final_memory:.2f} MB")
print(f"  └─ 品質スコア: {readiness_percentage:.0f}%")

print(f"\n🎯 Tableau導入推奨事項:")
if readiness_percentage >= 80:
    print(f"  🚀 すぐにTableauでの分析を開始できます！")
    print(f"  💡 推奨: ダッシュボード作成から始めてください")
elif readiness_percentage >= 60:
    print(f"  ✅ 基本的な分析が可能です")
    print(f"  💡 推奨: 簡単なチャートから作成してください")
else:
    print(f"  ⚠️  追加のデータ整備が推奨されます")
    print(f"  💡 推奨: 欠損値や異常値の手動確認をしてください")

print(f"\n📁 出力ファイル:")
print(f"  • メインデータ: {csv_filename}")
if 'excel_filename' in locals():
    print(f"  • Excelプレビュー: {excel_filename}")
if 'parquet_filename' in locals():
    print(f"  • 高効率形式: {parquet_filename}")
print(f"  • データ辞書: {dictionary_filename}")

print(f"\n🎉 プロフェッショナルなデータクリーニングが完了しました！")
print(f"=" * 60)

# ログの最終記録
logger.info(f"データクリーニング完全終了: {processing_time}, 品質スコア{readiness_percentage:.0f}%")

# 完了メッセージ
print(f"\n✨ お疲れ様でした！素晴らしいデータ分析を！✨")
