# =============================================================================
# ã€è¶…è©³ç´°ç‰ˆã€‘Pythonãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼š1è¡Œãšã¤å®Œå…¨è§£èª¬ã‚¬ã‚¤ãƒ‰
# æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘å®Ÿè·µãƒã‚¹ã‚¿ãƒ¼ã‚³ãƒ¼ã‚¹
# =============================================================================

# ğŸ“š ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ç›®çš„ã¨ä¾¡å€¤
# ãƒ»ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æå¯èƒ½ãªå½¢ã«å¤‰æ›ã™ã‚‹
# ãƒ»Tableauã§ã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã«æœ€é©åŒ–ã™ã‚‹
# ãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’å‘ä¸Šã•ã›ã¦ä¿¡é ¼æ€§ã®é«˜ã„åˆ†æã‚’å®Ÿç¾ã™ã‚‹
# ãƒ»æ‰‹ä½œæ¥­ã‚’è‡ªå‹•åŒ–ã—ã¦åŠ¹ç‡æ€§ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹

print("ğŸš€ Pythonãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼šãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç‰ˆ")
print("=" * 60)

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ç’°å¢ƒè¨­å®šã€‘
# =============================================================================

# pandas: ãƒ‡ãƒ¼ã‚¿æ“ä½œã®ä¸­æ ¸ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆExcelæ“ä½œã®Pythonç‰ˆã®ã‚ˆã†ãªã‚‚ã®ï¼‰
import pandas as pd

# numpy: æ•°å€¤è¨ˆç®—ã¨NaNï¼ˆæ¬ æå€¤ï¼‰å‡¦ç†ã«ç‰¹åŒ–ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import numpy as np

# datetime: æ—¥ä»˜ãƒ»æ™‚åˆ»æ“ä½œç”¨ï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†æã§ã¯æ—¥ä»˜å‡¦ç†ãŒé »ç¹ï¼‰
import datetime as dt
from datetime import datetime

# warnings: Pythonå®Ÿè¡Œæ™‚ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ¶å¾¡
import warnings
# ignore: åˆ†æä½œæ¥­ä¸­ã®ä¸è¦ãªè­¦å‘Šã‚’éè¡¨ç¤ºã«ã—ã¦é›†ä¸­åŠ›ã‚’ç¶­æŒ
warnings.filterwarnings('ignore')

# logging: å‡¦ç†ã®è¨˜éŒ²ã‚’æ®‹ã™ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã¯å¿…é ˆï¼‰
import logging

# os: ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ“ä½œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å‡¦ç†ãªã©ï¼‰
import os

# sys: Pythonã‚·ã‚¹ãƒ†ãƒ é–¢é€£ã®æ“ä½œï¼ˆãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ãªã©ï¼‰
import sys

print("âœ… å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿å®Œäº†")

# ãƒ­ã‚°è¨­å®šï¼ˆå‡¦ç†ã®è¨˜éŒ²ã‚’æ®‹ã™ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªæ‰‹æ³•ï¼‰
logging.basicConfig(
    level=logging.INFO,                    # INFO ãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ã‚’å‡ºåŠ›
    format='%(asctime)s - %(levelname)s - %(message)s',  # ãƒ­ã‚°ã®å½¢å¼ã‚’å®šç¾©
    handlers=[
        logging.FileHandler('data_cleaning.log'),  # ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ­ã‚°ã‚’ä¿å­˜
        logging.StreamHandler()                     # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚ãƒ­ã‚°ã‚’è¡¨ç¤º
    ]
)
# ãƒ­ã‚¬ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆä»¥é™ã®ãƒ­ã‚°å‡ºåŠ›ã§ä½¿ç”¨ï¼‰
logger = logging.getLogger(__name__)

# ç¾åœ¨æ™‚åˆ»ã‚’è¨˜éŒ²ï¼ˆå‡¦ç†é–‹å§‹æ™‚åˆ»ã¨ã—ã¦ï¼‰
start_time = datetime.now()
logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†é–‹å§‹: {start_time}")

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³2: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åˆæœŸæ¤œè¨¼ã€‘
# =============================================================================

print("\nğŸ“– STEP 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æ¤œè¨¼")
print("-" * 50)

try:  # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°é–‹å§‹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—ã«å‚™ãˆã‚‹ï¼‰
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰
    file_path = 'sample_data.csv'  # å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    
    # encoding='utf-8': æ—¥æœ¬èªæ–‡å­—åŒ–ã‘ã‚’é˜²ã
    # low_memory=False: å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ãƒ‡ãƒ¼ã‚¿å‹æ¨å®šã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
    df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)
    
    logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {file_path}")
    
except FileNotFoundError:  # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®å‡¦ç†
    logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
    print(f"ã‚¨ãƒ©ãƒ¼: {file_path} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)  # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ï¼ˆè‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ï¼‰
    
except UnicodeDecodeError:  # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
    logger.warning("æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã€‚Shift_JISã§å†è©¦è¡Œã—ã¾ã™ã€‚")
    try:
        # æ—¥æœ¬èªCSVã§ã‚ˆãã‚ã‚‹Shift_JISå½¢å¼ã§å†è©¦è¡Œ
        df = pd.read_csv(file_path, encoding='shift_jis', low_memory=False)
        logger.info("âœ… Shift_JISã§ã®èª­ã¿è¾¼ã¿æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        sys.exit(1)
        
except Exception as e:  # ãã®ä»–ã®äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼
    logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³3: ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±åˆ†æã€‘
# =============================================================================

print(f"\nğŸ” ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±åˆ†æ")
print("=" * 50)

# ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ï¼ˆè¡Œæ•°ã¨åˆ—æ•°ï¼‰ã‚’å–å¾—
rows, columns = df.shape
print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {rows:,}è¡Œ Ã— {columns}åˆ—")  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¦‹ã‚„ã™ãè¡¨ç¤º

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèªï¼ˆå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯é‡è¦ãªæŒ‡æ¨™ï¼‰
memory_usage_mb = df.memory_usage(deep=True).sum() / 1024 / 1024  # ãƒã‚¤ãƒˆã‚’MBã«å¤‰æ›
print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_usage_mb:.2f} MB")

# å„åˆ—ã®åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤º
print(f"\nğŸ“‹ åˆ—ã®ä¸€è¦§ ({len(df.columns)}å€‹):")
for i, column in enumerate(df.columns, 1):  # enumerate: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»˜ããƒ«ãƒ¼ãƒ—
    # å„åˆ—ã®æƒ…å ±ã‚’æ•´ç†ã—ã¦è¡¨ç¤º
    dtype = df[column].dtype                    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—
    null_count = df[column].isnull().sum()     # æ¬ æå€¤æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    null_percentage = (null_count / len(df)) * 100  # æ¬ æå€¤ã®å‰²åˆã‚’è¨ˆç®—
    unique_count = df[column].nunique()        # ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã®æ•°ã‚’å–å¾—
    
    # æƒ…å ±ã‚’è¦‹ã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›
    print(f"  {i:2d}. {column:<25} | å‹: {str(dtype):<12} | æ¬ æ: {null_count:>6}å€‹ ({null_percentage:>5.1f}%) | ãƒ¦ãƒ‹ãƒ¼ã‚¯: {unique_count:>8}å€‹")

# ãƒ‡ãƒ¼ã‚¿å‹ã®åˆ†å¸ƒã‚’ç¢ºèªï¼ˆã©ã‚“ãªç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ãŒå¤šã„ã‹ã‚’æŠŠæ¡ï¼‰
print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å‹ã®åˆ†å¸ƒ:")
dtype_counts = df.dtypes.value_counts()  # ãƒ‡ãƒ¼ã‚¿å‹ã”ã¨ã®åˆ—æ•°ã‚’é›†è¨ˆ
for dtype, count in dtype_counts.items():
    print(f"  {str(dtype):<15}: {count:>3}åˆ—")

# æ¬ æå€¤ã®å…¨ä½“çŠ¶æ³ã‚’ç¢ºèª
total_missing = df.isnull().sum().sum()  # å…¨ä½“ã®æ¬ æå€¤æ•°
total_cells = df.shape[0] * df.shape[1]  # å…¨ã‚»ãƒ«æ•°
missing_percentage = (total_missing / total_cells) * 100  # æ¬ æå€¤ã®å…¨ä½“å‰²åˆ

print(f"\nğŸš¨ æ¬ æå€¤ã®å…¨ä½“çŠ¶æ³:")
print(f"  ç·æ¬ æå€¤æ•°: {total_missing:,}å€‹")
print(f"  å…¨ä½“ã«å ã‚ã‚‹å‰²åˆ: {missing_percentage:.2f}%")

# æœ€åˆã®5è¡Œã¨æœ€å¾Œã®5è¡Œã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒ¼ã‚¿ã®ä¸­èº«ã‚’ç¢ºèªï¼‰
print(f"\nğŸ‘€ ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5è¡Œ:")
print(df.head())  # head(): æœ€åˆã®5è¡Œã‚’è¡¨ç¤º

print(f"\nğŸ‘€ ãƒ‡ãƒ¼ã‚¿ã®æœ«å°¾5è¡Œ:")
print(df.tail())  # tail(): æœ€å¾Œã®5è¡Œã‚’è¡¨ç¤º

# æ•°å€¤åˆ—ã®åŸºæœ¬çµ±è¨ˆæƒ…å ±
numeric_columns = df.select_dtypes(include=[np.number]).columns  # æ•°å€¤å‹ã®åˆ—ã®ã¿é¸æŠ
if len(numeric_columns) > 0:  # æ•°å€¤åˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ
    print(f"\nğŸ“Š æ•°å€¤åˆ—ã®åŸºæœ¬çµ±è¨ˆ ({len(numeric_columns)}åˆ—):")
    print(df[numeric_columns].describe())  # describe(): å¹³å‡ã€æ¨™æº–åå·®ã€æœ€å°å€¤ã€æœ€å¤§å€¤ãªã©
else:
    print(f"\nâš ï¸  æ•°å€¤åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³4: ãƒ‡ãƒ¼ã‚¿å“è³ªã®è©³ç´°åˆ†æã€‘
# =============================================================================

print(f"\nğŸ”¬ STEP 2: ãƒ‡ãƒ¼ã‚¿å“è³ªã®è©³ç´°åˆ†æ")
print("-" * 50)

# é‡è¤‡è¡Œã®è©³ç´°åˆ†æ
duplicate_count = df.duplicated().sum()  # é‡è¤‡è¡Œã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
if duplicate_count > 0:  # é‡è¤‡ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
    duplicate_percentage = (duplicate_count / len(df)) * 100  # é‡è¤‡ã®å‰²åˆ
    print(f"ğŸ”´ é‡è¤‡è¡Œæ¤œå‡º: {duplicate_count:,}è¡Œ ({duplicate_percentage:.2f}%)")
    
    # é‡è¤‡è¡Œã®è©³ç´°ã‚’è¡¨ç¤º
    duplicate_rows = df[df.duplicated(keep=False)]  # keep=False: é‡è¤‡è¡Œã‚’å…¨ã¦è¡¨ç¤º
    print(f"   é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(duplicate_rows.drop_duplicates())}ç¨®é¡")
    
else:
    print(f"âœ… é‡è¤‡è¡Œ: ãªã—")

# å„åˆ—ã®è©³ç´°ãªå“è³ªãƒã‚§ãƒƒã‚¯
print(f"\nğŸ” å„åˆ—ã®å“è³ªåˆ†æ:")
for column in df.columns:  # å…¨ã¦ã®åˆ—ã‚’é †æ¬¡ãƒã‚§ãƒƒã‚¯
    
    print(f"\nğŸ“ åˆ—: '{column}'")
    print(f"   â”œâ”€ ãƒ‡ãƒ¼ã‚¿å‹: {df[column].dtype}")
    
    # æ¬ æå€¤ã®è©³ç´°
    null_count = df[column].isnull().sum()
    null_percentage = (null_count / len(df)) * 100
    print(f"   â”œâ”€ æ¬ æå€¤: {null_count:,}å€‹ ({null_percentage:.1f}%)")
    
    # ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã®åˆ†æ
    unique_count = df[column].nunique()
    unique_percentage = (unique_count / len(df)) * 100
    print(f"   â”œâ”€ ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: {unique_count:,}å€‹ ({unique_percentage:.1f}%)")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ—ã®å ´åˆã®è¿½åŠ åˆ†æ
    if df[column].dtype == 'object':  # æ–‡å­—åˆ—ï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰å‹ã®å ´åˆ
        # æœ€é »å€¤ã‚’ç¢ºèª
        try:
            mode_value = df[column].mode()[0] if len(df[column].mode()) > 0 else "N/A"
            mode_count = (df[column] == mode_value).sum() if mode_value != "N/A" else 0
            print(f"   â”œâ”€ æœ€é »å€¤: '{mode_value}' ({mode_count}å›)")
        except:
            print(f"   â”œâ”€ æœ€é »å€¤: å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        
        # æ–‡å­—åˆ—é•·ã®åˆ†æ
        str_lengths = df[column].astype(str).str.len()  # å„å€¤ã®æ–‡å­—æ•°ã‚’è¨ˆç®—
        print(f"   â”œâ”€ æ–‡å­—æ•°: æœ€çŸ­{str_lengths.min()} ï½ æœ€é•·{str_lengths.max()}æ–‡å­—")
        
        # ç©ºæ–‡å­—åˆ—ã‚„ç©ºç™½ã®ã¿ã®å€¤ã‚’ãƒã‚§ãƒƒã‚¯
        empty_strings = (df[column].astype(str).str.strip() == '').sum()  # ç©ºç™½ã‚’é™¤å»ã—ã¦ç©ºæ–‡å­—åˆ—ã‹ãƒã‚§ãƒƒã‚¯
        if empty_strings > 0:
            print(f"   â”œâ”€ âš ï¸  ç©ºæ–‡å­—åˆ—: {empty_strings}å€‹")
        
    # æ•°å€¤åˆ—ã®å ´åˆã®è¿½åŠ åˆ†æ
    elif np.issubdtype(df[column].dtype, np.number):  # æ•°å€¤å‹ã®å ´åˆ
        # åŸºæœ¬çµ±è¨ˆ
        print(f"   â”œâ”€ æœ€å°å€¤: {df[column].min()}")
        print(f"   â”œâ”€ æœ€å¤§å€¤: {df[column].max()}")
        print(f"   â”œâ”€ å¹³å‡å€¤: {df[column].mean():.2f}")
        print(f"   â”œâ”€ ä¸­å¤®å€¤: {df[column].median():.2f}")
        
        # ç•°å¸¸å€¤ã®æ¤œå‡ºï¼ˆIQRæ³•ï¼‰
        Q1 = df[column].quantile(0.25)      # ç¬¬1å››åˆ†ä½æ•°
        Q3 = df[column].quantile(0.75)      # ç¬¬3å››åˆ†ä½æ•°
        IQR = Q3 - Q1                       # å››åˆ†ä½ç¯„å›²
        lower_bound = Q1 - 1.5 * IQR        # ç•°å¸¸å€¤ã®ä¸‹é™
        upper_bound = Q3 + 1.5 * IQR        # ç•°å¸¸å€¤ã®ä¸Šé™
        
        # ç•°å¸¸å€¤ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
        outlier_count = len(outliers)
        if outlier_count > 0:
            outlier_percentage = (outlier_count / len(df)) * 100
            print(f"   â””â”€ âš ï¸  ç•°å¸¸å€¤: {outlier_count}å€‹ ({outlier_percentage:.1f}%) ç¯„å›²å¤–: {lower_bound:.1f}ï½{upper_bound:.1f}")
        else:
            print(f"   â””â”€ âœ… ç•°å¸¸å€¤: ãªã—")
    
    else:
        print(f"   â””â”€ ãã®ä»–ã®å‹")

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³5: åˆ—åã®æ¨™æº–åŒ–ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€‘
# =============================================================================

print(f"\nğŸ·ï¸ STEP 3: åˆ—åã®æ¨™æº–åŒ–")
print("-" * 50)

# å…ƒã®åˆ—åã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå‡¦ç†å‰å¾Œã®æ¯”è¼ƒç”¨ï¼‰
original_columns = df.columns.tolist()
logger.info(f"å…ƒã®åˆ—å: {original_columns}")

print("åˆ—åæ¨™æº–åŒ–ã®å‡¦ç†:")

# 1. å°æ–‡å­—ã¸ã®å¤‰æ›
print("  1ï¸âƒ£ å°æ–‡å­—å¤‰æ›ä¸­...")
df.columns = df.columns.str.lower()
print(f"     çµæœä¾‹: {list(df.columns[:3])}...")  # æœ€åˆã®3åˆ—ã®ã¿è¡¨ç¤º

# 2. ç©ºç™½ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›
print("  2ï¸âƒ£ ç©ºç™½ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«å¤‰æ›ä¸­...")
df.columns = df.columns.str.replace(' ', '_')
print(f"     çµæœä¾‹: {list(df.columns[:3])}...")

# 3. é€£ç¶šã™ã‚‹ç©ºç™½ã‚’å˜ä¸€ã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«å¤‰æ›
print("  3ï¸âƒ£ é€£ç¶šç©ºç™½ã®æ•´ç†ä¸­...")
df.columns = df.columns.str.replace(r'\s+', '_', regex=True)  # æ­£è¦è¡¨ç¾ã§è¤‡æ•°ç©ºç™½ã‚’å‡¦ç†

# 4. ç‰¹æ®Šæ–‡å­—ã®é™¤å»ï¼ˆè‹±æ•°å­—ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ä»¥å¤–ã‚’å‰Šé™¤ï¼‰
print("  4ï¸âƒ£ ç‰¹æ®Šæ–‡å­—é™¤å»ä¸­...")
df.columns = df.columns.str.replace(r'[^a-zA-Z0-9_]', '', regex=True)

# 5. é€£ç¶šã™ã‚‹ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å˜ä¸€ã«å¤‰æ›
print("  5ï¸âƒ£ é€£ç¶šã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®æ•´ç†ä¸­...")
df.columns = df.columns.str.replace(r'_+', '_', regex=True)

# 6. å…ˆé ­ãƒ»æœ«å°¾ã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’é™¤å»
print("  6ï¸âƒ£ å‰å¾Œã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®é™¤å»ä¸­...")
df.columns = df.columns.str.strip('_')

# 7. ç©ºã®åˆ—åã‚„æ•°å­—ã®ã¿ã®åˆ—åã¸ã®å¯¾å¿œ
print("  7ï¸âƒ£ å•é¡Œã®ã‚ã‚‹åˆ—åã®ä¿®æ­£ä¸­...")
for i, column in enumerate(df.columns):
    if column == '' or column.isdigit():  # ç©ºæ–‡å­—åˆ—ã¾ãŸã¯æ•°å­—ã®ã¿ã®å ´åˆ
        new_name = f'column_{i+1}'       # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’ä»˜ä¸
        df.columns.values[i] = new_name   # åˆ—åã‚’å¤‰æ›´
        print(f"     âš ï¸  å•é¡Œã®ã‚ã‚‹åˆ—åã‚’ä¿®æ­£: '{original_columns[i]}' â†’ '{new_name}'")

# 8. é‡è¤‡ã™ã‚‹åˆ—åã®å‡¦ç†
print("  8ï¸âƒ£ é‡è¤‡åˆ—åã®å‡¦ç†ä¸­...")
column_counts = {}  # åˆ—åã®å‡ºç¾å›æ•°ã‚’è¨˜éŒ²ã™ã‚‹è¾æ›¸
new_columns = []    # æ–°ã—ã„åˆ—åã®ãƒªã‚¹ãƒˆ

for column in df.columns:
    if column in column_counts:  # æ—¢ã«å­˜åœ¨ã™ã‚‹åˆ—åã®å ´åˆ
        column_counts[column] += 1
        new_column_name = f"{column}_{column_counts[column]}"  # é€£ç•ªã‚’ä»˜ä¸
        new_columns.append(new_column_name)
        print(f"     âš ï¸  é‡è¤‡åˆ—åã‚’ä¿®æ­£: '{column}' â†’ '{new_column_name}'")
    else:
        column_counts[column] = 1
        new_columns.append(column)

df.columns = new_columns  # ä¿®æ­£ã—ãŸåˆ—åã‚’é©ç”¨

# å¤‰æ›´çµæœã®è¡¨ç¤º
print(f"\nğŸ“Š åˆ—åæ¨™æº–åŒ–ã®çµæœ:")
print(f"  å‡¦ç†å‰åˆ—æ•°: {len(original_columns)}")
print(f"  å‡¦ç†å¾Œåˆ—æ•°: {len(df.columns)}")

changes_made = 0  # å¤‰æ›´ãŒã‚ã£ãŸåˆ—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
print(f"\nğŸ”„ å¤‰æ›´ã•ã‚ŒãŸåˆ—å:")
for old, new in zip(original_columns, df.columns):
    if old != new:  # å¤‰æ›´ãŒã‚ã£ãŸå ´åˆã®ã¿è¡¨ç¤º
        print(f"  '{old}' â†’ '{new}'")
        changes_made += 1

if changes_made == 0:
    print(f"  å¤‰æ›´ãªã—ï¼ˆå…¨ã¦ã®åˆ—åãŒæ—¢ã«æ¨™æº–å½¢å¼ã§ã—ãŸï¼‰")
else:
    print(f"  åˆè¨ˆ {changes_made} åˆ—ã®åå‰ã‚’å¤‰æ›´ã—ã¾ã—ãŸ")

logger.info(f"åˆ—åæ¨™æº–åŒ–å®Œäº†: {changes_made}åˆ—ã‚’å¤‰æ›´")

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³6: é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æã¨å‰Šé™¤ã€‘
# =============================================================================

print(f"\nğŸ—‘ï¸ STEP 4: é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†")
print("-" * 50)

# é‡è¤‡ãƒã‚§ãƒƒã‚¯å‰ã®çŠ¶æ…‹ã‚’è¨˜éŒ²
initial_rows = len(df)
initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024

print(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯é–‹å§‹:")
print(f"  ğŸ“Š å‡¦ç†å‰ãƒ‡ãƒ¼ã‚¿: {initial_rows:,}è¡Œ")
print(f"  ğŸ’¾ å‡¦ç†å‰ãƒ¡ãƒ¢ãƒª: {initial_memory:.2f} MB")

# 1. å®Œå…¨é‡è¤‡è¡Œã®æ¤œå‡º
print(f"\nğŸ” å®Œå…¨é‡è¤‡è¡Œã®åˆ†æ:")
duplicate_mask = df.duplicated(keep=False)  # keep=False: é‡è¤‡è¡Œå…¨ã¦ã‚’Trueã«ã™ã‚‹
duplicate_rows = df[duplicate_mask]          # é‡è¤‡è¡Œã®ã¿ã‚’æŠ½å‡º
unique_duplicate_patterns = df[duplicate_mask].drop_duplicates()  # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—

if len(duplicate_rows) > 0:  # é‡è¤‡ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
    print(f"  ğŸ”´ é‡è¤‡è¡Œç™ºè¦‹: {len(duplicate_rows):,}è¡Œ")
    print(f"  ğŸ“‹ é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(unique_duplicate_patterns):,}ç¨®é¡")
    
    # é‡è¤‡ã®å½±éŸ¿åº¦ã‚’åˆ†æ
    duplicate_percentage = (len(duplicate_rows) / initial_rows) * 100
    print(f"  ğŸ“ˆ é‡è¤‡ç‡: {duplicate_percentage:.2f}%")
    
    # æœ€ã‚‚å¤šã„é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¡¨ç¤º
    if len(unique_duplicate_patterns) > 0:
        print(f"\n  ğŸ“‹ é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¾‹ï¼ˆæœ€åˆã®3è¡Œï¼‰:")
        for i, (idx, row) in enumerate(unique_duplicate_patterns.head(3).iterrows()):
            duplicate_count = (df == row).all(axis=1).sum()  # ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡è¤‡æ•°
            print(f"    ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}: {duplicate_count}å›é‡è¤‡")
            # é‡è¦ãªåˆ—ã ã‘è¡¨ç¤ºï¼ˆå…¨åˆ—ã¯å¤šã™ãã‚‹ãŸã‚ï¼‰
            display_columns = df.columns[:3] if len(df.columns) >= 3 else df.columns
            for col in display_columns:
                print(f"      {col}: {row[col]}")
            if i < 2:  # æœ€å¾Œä»¥å¤–ã¯åŒºåˆ‡ã‚Šç·š
                print(f"      " + "-" * 30)
else:
    print(f"  âœ… é‡è¤‡è¡Œãªã—")

# 2. ç‰¹å®šåˆ—ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ“ã‚¸ãƒã‚¹ã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆï¼‰
# ä¾‹ï¼šIDã‚„ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãªã©ã€ä¸€æ„ã§ã‚ã‚‹ã¹ãåˆ—
key_columns_candidates = []  # ä¸€æ„ã§ã‚ã‚‹ã¹ãå¯èƒ½æ€§ã®ã‚ã‚‹åˆ—ã‚’æ¢ã™

for column in df.columns:
    # åˆ—åã«id, email, code, numberãªã©ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ã‚­ãƒ¼åˆ—å€™è£œã¨ã™ã‚‹
    if any(keyword in column.lower() for keyword in ['id', 'email', 'code', 'number', 'key']):
        unique_ratio = df[column].nunique() / len(df)  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡ã‚’è¨ˆç®—
        if unique_ratio > 0.95:  # 95%ä»¥ä¸ŠãŒãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚‰ã‚­ãƒ¼åˆ—å€™è£œ
            key_columns_candidates.append(column)

if key_columns_candidates:
    print(f"\nğŸ”‘ ãƒ“ã‚¸ãƒã‚¹ã‚­ãƒ¼åˆ—ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯:")
    for key_column in key_columns_candidates:
        key_duplicates = df[key_column].duplicated().sum()
        if key_duplicates > 0:
            print(f"  âš ï¸  '{key_column}': {key_duplicates}ä»¶ã®é‡è¤‡")
            # é‡è¤‡å€¤ã®ä¾‹ã‚’è¡¨ç¤º
            duplicate_values = df[df[key_column].duplicated(keep=False)][key_column].unique()[:3]
            print(f"       é‡è¤‡å€¤ä¾‹: {list(duplicate_values)}")
        else:
            print(f"  âœ… '{key_column}': é‡è¤‡ãªã—")

# 3. é‡è¤‡è¡Œã®å‰Šé™¤å®Ÿè¡Œ
print(f"\nğŸš® é‡è¤‡è¡Œå‰Šé™¤ã®å®Ÿè¡Œ:")

if len(duplicate_rows) > 0:
    # é‡è¤‡è¡Œå‰Šé™¤å‰ã«ç¢ºèª
    print(f"  â“ {len(duplicate_rows):,}è¡Œã®é‡è¤‡ã‚’å‰Šé™¤ã—ã¾ã™...")
    
    # å‰Šé™¤å®Ÿè¡Œï¼ˆæœ€åˆã®å‡ºç¾ã‚’ä¿æŒã€å¾Œç¶šã‚’å‰Šé™¤ï¼‰
    df_before_dedup = df.copy()  # å‰Šé™¤å‰ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    df = df.drop_duplicates(keep='first')  # keep='first': æœ€åˆã®å‡ºç¾ã‚’ä¿æŒ
    
    # å‰Šé™¤çµæœã®ç¢ºèª
    final_rows = len(df)
    removed_rows = initial_rows - final_rows
    space_saved = (df_before_dedup.memory_usage(deep=True).sum() - df.memory_usage(deep=True).sum()) / 1024 / 1024
    
    print(f"  âœ… é‡è¤‡å‰Šé™¤å®Œäº†!")
    print(f"    â”œâ”€ å‰Šé™¤å‰: {initial_rows:,}è¡Œ")
    print(f"    â”œâ”€ å‰Šé™¤å¾Œ: {final_rows:,}è¡Œ")
    print(f"    â”œâ”€ å‰Šé™¤æ•°: {removed_rows:,}è¡Œ")
    print(f"    â”œâ”€ å‰Šé™¤ç‡: {(removed_rows/initial_rows)*100:.2f}%")
    print(f"    â””â”€ ãƒ¡ãƒ¢ãƒªç¯€ç´„: {space_saved:.2f} MB")
    
    logger.info(f"é‡è¤‡å‰Šé™¤: {removed_rows}è¡Œå‰Šé™¤ã€{space_saved:.2f}MBç¯€ç´„")
    
else:
    print(f"  â„¹ï¸  å‰Šé™¤å¯¾è±¡ã®é‡è¤‡è¡ŒãŒãªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³7: ãƒ‡ãƒ¼ã‚¿å‹ã®è©³ç´°å¤‰æ›ã€‘
# =============================================================================

print(f"\nğŸ”„ STEP 5: ãƒ‡ãƒ¼ã‚¿å‹ã®æœ€é©åŒ–å¤‰æ›")
print("-" * 50)

# å¤‰æ›å‰ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’è¨˜éŒ²
original_dtypes = df.dtypes.copy()
conversion_results = []  # å¤‰æ›çµæœã‚’è¨˜éŒ²ã™ã‚‹ãƒªã‚¹ãƒˆ

print(f"ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã®é–‹å§‹:")
print(f"  ğŸ“Š å¯¾è±¡åˆ—æ•°: {len(df.columns)}åˆ—")

# 1. æ—¥ä»˜åˆ—ã®è‡ªå‹•æ¤œå‡ºã¨å¤‰æ›
print(f"\nğŸ“… æ—¥ä»˜åˆ—ã®æ¤œå‡ºã¨å¤‰æ›:")
date_conversion_count = 0  # å¤‰æ›æˆåŠŸæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ

for column in df.columns:
    # åˆ—åã«æ—¥ä»˜ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    date_keywords = ['date', 'time', 'created', 'updated', 'modified', 'birth', 'expire']
    is_potential_date = any(keyword in column.lower() for keyword in date_keywords)
    
    if is_potential_date and df[column].dtype == 'object':  # æ–‡å­—åˆ—å‹ã§æ—¥ä»˜å€™è£œã®å ´åˆ
        print(f"  ğŸ” '{column}' ã®æ—¥ä»˜å¤‰æ›ã‚’è©¦è¡Œä¸­...")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦æ—¥ä»˜ã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯
        sample_values = df[column].dropna().head(5).tolist()
        print(f"    ã‚µãƒ³ãƒ—ãƒ«å€¤: {sample_values}")
        
        try:
            # ã¾ãšæ•°è¡Œã§ãƒ†ã‚¹ãƒˆå¤‰æ›
            test_series = df[column].head(100).dropna()
            test_converted = pd.to_datetime(test_series, infer_datetime_format=True, errors='coerce')
            
            # å¤‰æ›æˆåŠŸç‡ã‚’ãƒã‚§ãƒƒã‚¯
            success_rate = (test_converted.notna().sum() / len(test_series)) if len(test_series) > 0 else 0
            
            if success_rate >= 0.8:  # 80%ä»¥ä¸ŠãŒæ­£å¸¸ã«å¤‰æ›ã§ãã‚‹å ´åˆã®ã¿å®Ÿè¡Œ
                # å…¨ä½“ã«é©ç”¨
                df[column] = pd.to_datetime(df[column], infer_datetime_format=True, errors='coerce')
                
                # å¤‰æ›çµæœã®ç¢ºèª
                valid_dates = df[column].notna().sum()
                total_values = len(df[column])
                final_success_rate = (valid_dates / total_values) * 100
                
                print(f"    âœ… å¤‰æ›æˆåŠŸ! æœ‰åŠ¹æ—¥ä»˜: {valid_dates:,}/{total_values:,} ({final_success_rate:.1f}%)")
                date_conversion_count += 1
                conversion_results.append(f"âœ… {column}: object â†’ datetime64")
                
                # æ—¥ä»˜ç¯„å›²ã®ç¢ºèª
                if valid_dates > 0:
                    min_date = df[column].min()
                    max_date = df[column].max()
                    print(f"    ğŸ“Š æ—¥ä»˜ç¯„å›²: {min_date} ï½ {max_date}")
            else:
                print(f"    âŒ å¤‰æ›æˆåŠŸç‡ãŒä½ã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ— ({success_rate*100:.1f}%)")
                
        except Exception as e:
            print(f"    âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...")

print(f"  ğŸ“ˆ æ—¥ä»˜å¤‰æ›çµæœ: {date_conversion_count}åˆ—å¤‰æ›å®Œäº†")

# 2. æ•°å€¤åˆ—ã®æ¤œå‡ºã¨å¤‰æ›
print(f"\nğŸ”¢ æ•°å€¤åˆ—ã®æ¤œå‡ºã¨å¤‰æ›:")
numeric_conversion_count = 0

# æ–‡å­—åˆ—å‹ã®åˆ—ã®ã¿ã‚’å¯¾è±¡
string_columns = df.select_dtypes(include=['object']).columns

for column in string_columns:
    if column in [col for col in df.columns if df[col].dtype == 'datetime64[ns]']:
        continue  # æ—¢ã«æ—¥ä»˜å¤‰æ›ã•ã‚ŒãŸåˆ—ã¯ã‚¹ã‚­ãƒƒãƒ—
    
    print(f"  ğŸ” '{column}' ã®æ•°å€¤å¤‰æ›ã‚’è©¦è¡Œä¸­...")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    sample_values = df[column].dropna().head(5).tolist()
    print(f"    ã‚µãƒ³ãƒ—ãƒ«å€¤: {sample_values}")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆé€šè²¨è¨˜å·ã€ã‚«ãƒ³ãƒãªã©ã‚’é™¤å»ï¼‰
        cleaned_series = df[column].astype(str).copy()
        
        # ã‚ˆãã‚ã‚‹æ–‡å­—åˆ—ã‚’é™¤å»
        print(f"    ğŸ§¹ æ–‡å­—åˆ—ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
        
        # ã‚«ãƒ³ãƒã®é™¤å»
        cleaned_series = cleaned_series.str.replace(',', '')
        # é€šè²¨è¨˜å·ã®é™¤å»
        cleaned_series = cleaned_series.str.replace('Â¥', '').str.replace('$', '').str.replace('â‚¬', '')
        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¨˜å·ã®é™¤å»
        cleaned_series = cleaned_series.str.replace('%', '')
        # å‰å¾Œã®ç©ºç™½é™¤å»
        cleaned_series = cleaned_series.str.strip()
        # å…¨è§’æ•°å­—ã‚’åŠè§’ã«å¤‰æ›ï¼ˆæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
        cleaned_series = cleaned_series.str.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
        
        # æ•°å€¤å¤‰æ›ã®ãƒ†ã‚¹ãƒˆ
        test_series = cleaned_series.head(100)
        test_converted = pd.to_numeric(test_series, errors='coerce')
        
        # å¤‰æ›æˆåŠŸç‡ã‚’ãƒã‚§ãƒƒã‚¯
        valid_numbers = test_converted.notna().sum()
        success_rate = (valid_numbers / len(test_series)) if len(test_series) > 0 else 0
        
        if success_rate >= 0.8:  # 80%ä»¥ä¸ŠãŒæ•°å€¤ã«å¤‰æ›å¯èƒ½ãªå ´åˆ
            # å…¨ä½“ã«é©ç”¨
            df[column] = pd.to_numeric(cleaned_series, errors='coerce')
            
            # çµæœã®ç¢ºèª
            final_valid = df[column].notna().sum()
            total_count = len(df[column])
            final_success_rate = (final_valid / total_count) * 100
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã®æœ€é©åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
            if df[column].notna().any():  # æœ‰åŠ¹ãªæ•°å€¤ãŒã‚ã‚‹å ´åˆ
                # æ•´æ•°ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
                if df[column].dropna().apply(lambda x: x == int(x)).all():
                    # æ•´æ•°ã®å ´åˆã€æœ€é©ãªintå‹ã‚’é¸æŠ
                    min_val = df[column].min()
                    max_val = df[column].max()
                    
                    if min_val >= 0:  # éè² æ•´æ•°ã®å ´åˆ
                        if max_val <= 255:
                            df[column] = df[column].astype('uint8')
                            dtype_name = 'uint8'
                        elif max_val <= 65535:
                            df[column] = df[column].astype('uint16')
                            dtype_name = 'uint16'
                        else:
                            df[column] = df[column].astype('uint32')
                            dtype_name = 'uint32'
                    else:  # ç¬¦å·ä»˜ãæ•´æ•°ã®å ´åˆ
                        if min_val >= -128 and max_val <= 127:
                            df[column] = df[column].astype('int8')
                            dtype_name = 'int8'
                        elif min_val >= -32768 and max_val <= 32767:
                            df[column] = df[column].astype('int16')
                            dtype_name = 'int16'
                        else:
                            df[column] = df[column].astype('int32')
                            dtype_name = 'int32'
                else:
                    # æµ®å‹•å°æ•°ç‚¹æ•°ã®å ´åˆ
                    df[column] = df[column].astype('float32')  # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã§float32ä½¿ç”¨
                    dtype_name = 'float32'
                
                print(f"    âœ… å¤‰æ›æˆåŠŸ! æœ‰åŠ¹æ•°å€¤: {final_valid:,}/{total_count:,} ({final_success_rate:.1f}%) â†’ {dtype_name}")
                numeric_conversion_count += 1
                conversion_results.append(f"âœ… {column}: object â†’ {dtype_name}")
                
                # æ•°å€¤ç¯„å›²ã®ç¢ºèª
                if final_valid > 0:
                    min_val = df[column].min()
                    max_val = df[column].max()
                    mean_val = df[column].mean()
                    print(f"    ğŸ“Š æ•°å€¤ç¯„å›²: {min_val:.2f} ï½ {max_val:.2f} (å¹³å‡: {mean_val:.2f})")
            else:
                print(f"    âŒ æœ‰åŠ¹ãªæ•°å€¤ãŒã‚ã‚Šã¾ã›ã‚“")
        else:
            print(f"    âŒ å¤‰æ›æˆåŠŸç‡ãŒä½ã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ— ({success_rate*100:.1f}%)")
            
    except Exception as e:
        print(f"    âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...")

print(f"  ğŸ“ˆ æ•°å€¤å¤‰æ›çµæœ: {numeric_conversion_count}åˆ—å¤‰æ›å®Œäº†")

# 3. ã‚«ãƒ†ã‚´ãƒªå‹ã¸ã®æœ€é©åŒ–
print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªå‹ã¸ã®æœ€é©åŒ–:")
category_conversion_count = 0

# æ®‹ã‚Šã®æ–‡å­—åˆ—åˆ—ã‚’ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
remaining_string_columns = df.select_dtypes(include=['object']).columns

for column in remaining_string_columns:
    unique_count = df[column].nunique()
    total_count = len(df[column])
    unique_ratio = unique_count / total_count if total_count > 0 else 0
    
    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡ãŒ50%æœªæº€ã®å ´åˆã€ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„åŠ¹æœå¤§ï¼‰
    if unique_ratio < 0.5 and unique_count < 1000:  # 1000ç¨®é¡æœªæº€ã®ã‚«ãƒ†ã‚´ãƒª
        memory_before = df[column].memory_usage(deep=True) / 1024 / 1024
        
        df[column] = df[column].astype('category')
        
        memory_after = df[column].memory_usage(deep=True) / 1024 / 1024
        memory_saved = memory_before - memory_after
        
        print(f"  âœ… '{column}': ã‚«ãƒ†ã‚´ãƒªåŒ–å®Œäº†")
        print(f"    ğŸ“Š ã‚«ãƒ†ã‚´ãƒªæ•°: {unique_count:,}ç¨®é¡ ({unique_ratio*100:.1f}%)")
        print(f"    ğŸ’¾ ãƒ¡ãƒ¢ãƒªç¯€ç´„: {memory_saved:.2f} MB")
        
        category_conversion_count += 1
        conversion_results.append(f"âœ… {column}: object â†’ category")

print(f"  ğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªå¤‰æ›çµæœ: {category_conversion_count}åˆ—å¤‰æ›å®Œäº†")

# å¤‰æ›çµæœã®ã‚µãƒãƒªãƒ¼
print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã®ã‚µãƒãƒªãƒ¼:")
total_conversions = date_conversion_count + numeric_conversion_count + category_conversion_count
print(f"  ğŸ¯ ç·å¤‰æ›åˆ—æ•°: {total_conversions}åˆ—")
print(f"    â”œâ”€ æ—¥ä»˜å¤‰æ›: {date_conversion_count}åˆ—")
print(f"    â”œâ”€ æ•°å€¤å¤‰æ›: {numeric_conversion_count}åˆ—")
print(f"    â””â”€ ã‚«ãƒ†ã‚´ãƒªå¤‰æ›: {category_conversion_count}åˆ—")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¤‰åŒ–
current_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
memory_change = initial_memory - current_memory
print(f"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¤‰åŒ–: {initial_memory:.2f} MB â†’ {current_memory:.2f} MB")
if memory_change > 0:
    print(f"    âœ… {memory_change:.2f} MBå‰Šæ¸› ({(memory_change/initial_memory)*100:.1f}%æ¸›)")
else:
    print(f"    ğŸ“Š {abs(memory_change):.2f} MBå¢—åŠ ")

# å¤‰æ›´ã•ã‚ŒãŸåˆ—ã®è©³ç´°ãƒªã‚¹ãƒˆ
if conversion_results:
    print(f"\nğŸ”„ å¤‰æ›ã•ã‚ŒãŸåˆ—ã®è©³ç´°:")
    for result in conversion_results:
        print(f"    {result}")

logger.info(f"ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›å®Œäº†: {total_conversions}åˆ—å¤‰æ›ã€{memory_change:.2f}MBç¯€ç´„")

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³8: æ¬ æå€¤ã®é«˜åº¦ãªå‡¦ç†ã€‘
# =============================================================================

print(f"\nğŸ©¹ STEP 6: æ¬ æå€¤ã®æˆ¦ç•¥çš„å‡¦ç†")
print("-" * 50)

# æ¬ æå€¤å‡¦ç†å‰ã®çŠ¶æ…‹è¨˜éŒ²
missing_before_dict = df.isnull().sum().to_dict()  # åˆ—ã”ã¨ã®æ¬ æå€¤æ•°
total_missing_before = df.isnull().sum().sum()     # å…¨ä½“ã®æ¬ æå€¤æ•°

print(f"æ¬ æå€¤å‡¦ç†ã®é–‹å§‹:")
print(f"  ğŸ“Š å‡¦ç†å‰ç·æ¬ æå€¤: {total_missing_before:,}å€‹")

if total_missing_before == 0:
    print(f"  âœ… æ¬ æå€¤ãŒãªã„ãŸã‚ã€ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
else:
    print(f"  ğŸ¯ æˆ¦ç•¥çš„æ¬ æå€¤è£œå®Œã‚’å®Ÿè¡Œã—ã¾ã™")

    # æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
    print(f"\nğŸ” æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ:")
    
    # æ¬ æå€¤ãŒå¤šã„åˆ—ã®ç‰¹å®š
    high_missing_threshold = 0.5  # 50%ä»¥ä¸Šæ¬ æã—ã¦ã„ã‚‹åˆ—
    high_missing_columns = []
    
    for column in df.columns:
        missing_count = df[column].isnull().sum()
        missing_rate = missing_count / len(df)
        
        if missing_rate >= high_missing_threshold:
            high_missing_columns.append((column, missing_count, missing_rate))
            print(f"    âš ï¸  é«˜æ¬ æåˆ—: '{column}' - {missing_count:,}å€‹ ({missing_rate*100:.1f}%)")
    
    if high_missing_columns:
        print(f"    ğŸ’¡ æ¨å¥¨: é«˜æ¬ æåˆ—ã¯å‰Šé™¤ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
    
    # 1. æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†
    print(f"\nğŸ”¢ æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†:")
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    numeric_missing_handled = 0
    
    for column in numeric_columns:
        missing_count = df[column].isnull().sum()
        
        if missing_count > 0:
            missing_rate = missing_count / len(df)
            print(f"  ğŸ” '{column}': {missing_count:,}å€‹æ¬ æ ({missing_rate*100:.1f}%)")
            
            if missing_rate < high_missing_threshold:  # æ¬ æç‡ãŒ50%æœªæº€ã®å ´åˆã®ã¿è£œå®Œ
                # è£œå®Œæ–¹æ³•ã®æ±ºå®šï¼ˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã«åŸºã¥ãï¼‰
                try:
                    # å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’ç¢ºèª
                    Q1 = df[column].quantile(0.25)
                    Q3 = df[column].quantile(0.75)
                    IQR = Q3 - Q1
                    
                    # å¤–ã‚Œå€¤ã®é™¤å»å¾Œã®çµ±è¨ˆå€¤ã‚’è¨ˆç®—
                    clean_data = df[column][(df[column] >= Q1 - 1.5*IQR) & (df[column] <= Q3 + 1.5*IQR)]
                    
                    mean_val = clean_data.mean()
                    median_val = clean_data.median()
                    
                    # å¹³å‡ã¨ä¸­å¤®å€¤ã®å·®ã§è£œå®Œæ–¹æ³•ã‚’æ±ºå®š
                    if abs(mean_val - median_val) / median_val < 0.1:  # å·®ãŒ10%æœªæº€
                        # æ­£è¦åˆ†å¸ƒã«è¿‘ã„å ´åˆã¯å¹³å‡å€¤ã§è£œå®Œ
                        fill_value = mean_val
                        method = "å¹³å‡å€¤"
                    else:
                        # åã£ãŸåˆ†å¸ƒã®å ´åˆã¯ä¸­å¤®å€¤ã§è£œå®Œ
                        fill_value = median_val
                        method = "ä¸­å¤®å€¤"
                    
                    # è£œå®Œå®Ÿè¡Œ
                    df[column].fillna(fill_value, inplace=True)
                    
                    print(f"    âœ… {method}({fill_value:.2f})ã§è£œå®Œå®Œäº†")
                    numeric_missing_handled += 1
                    
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯0ã§è£œå®Œ
                    df[column].fillna(0, inplace=True)
                    print(f"    âš ï¸  ã‚¨ãƒ©ãƒ¼ã®ãŸã‚0ã§è£œå®Œ: {str(e)[:30]}...")
                    numeric_missing_handled += 1
            else:
                print(f"    âŒ æ¬ æç‡ãŒé«˜ã„ãŸã‚è£œå®Œã‚’ã‚¹ã‚­ãƒƒãƒ—")
    
    print(f"  ğŸ“ˆ æ•°å€¤åˆ—å‡¦ç†çµæœ: {numeric_missing_handled}åˆ—å‡¦ç†å®Œäº†")
    
    # 2. ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†
    print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†:")
    categorical_columns = df.select_dtypes(include=['object', 'category']).columns
    categorical_missing_handled = 0
    
    for column in categorical_columns:
        missing_count = df[column].isnull().sum()
        
        if missing_count > 0:
            missing_rate = missing_count / len(df)
            print(f"  ğŸ” '{column}': {missing_count:,}å€‹æ¬ æ ({missing_rate*100:.1f}%)")
            
            if missing_rate < high_missing_threshold:
                try:
                    # æœ€é »å€¤ã‚’å–å¾—
                    mode_values = df[column].mode()
                    
                    if len(mode_values) > 0 and pd.notna(mode_values[0]):
                        # æœ€é »å€¤ã§è£œå®Œ
                        fill_value = mode_values[0]
                        fill_count = (df[column] == fill_value).sum()
                        total_non_null = df[column].notna().sum()
                        frequency_rate = fill_count / total_non_null if total_non_null > 0 else 0
                        
                        df[column].fillna(fill_value, inplace=True)
                        
                        print(f"    âœ… æœ€é »å€¤('{fill_value}')ã§è£œå®Œå®Œäº†")
                        print(f"      ğŸ“Š æœ€é »å€¤ã®å‡ºç¾ç‡: {frequency_rate*100:.1f}%")
                        categorical_missing_handled += 1
                        
                    else:
                        # æœ€é »å€¤ãŒãªã„å ´åˆã¯'Unknown'ã§è£œå®Œ
                        df[column].fillna('Unknown', inplace=True)
                        print(f"    âœ… 'Unknown'ã§è£œå®Œå®Œäº†")
                        categorical_missing_handled += 1
                        
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯'Missing'ã§è£œå®Œ
                    df[column].fillna('Missing', inplace=True)
                    print(f"    âš ï¸  ã‚¨ãƒ©ãƒ¼ã®ãŸã‚'Missing'ã§è£œå®Œ: {str(e)[:30]}...")
                    categorical_missing_handled += 1
            else:
                print(f"    âŒ æ¬ æç‡ãŒé«˜ã„ãŸã‚è£œå®Œã‚’ã‚¹ã‚­ãƒƒãƒ—")
    
    print(f"  ğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ—å‡¦ç†çµæœ: {categorical_missing_handled}åˆ—å‡¦ç†å®Œäº†")
    
    # 3. æ—¥ä»˜åˆ—ã®æ¬ æå€¤å‡¦ç†
    print(f"\nğŸ“… æ—¥ä»˜åˆ—ã®æ¬ æå€¤å‡¦ç†:")
    datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns
    datetime_missing_handled = 0
    
    for column in datetime_columns:
        missing_count = df[column].isnull().sum()
        
        if missing_count > 0:
            missing_rate = missing_count / len(df)
            print(f"  ğŸ” '{column}': {missing_count:,}å€‹æ¬ æ ({missing_rate*100:.1f}%)")
            
            if missing_rate < high_missing_threshold:
                try:
                    # å‰æ–¹è£œå®Œã¨å¾Œæ–¹è£œå®Œã‚’çµ„ã¿åˆã‚ã›
                    original_missing = df[column].isnull().sum()
                    
                    # å‰æ–¹è£œå®Œï¼ˆå‰ã®å€¤ã§åŸ‹ã‚ã‚‹ï¼‰
                    df[column].fillna(method='ffill', inplace=True)
                    after_ffill_missing = df[column].isnull().sum()
                    
                    # å¾Œæ–¹è£œå®Œï¼ˆå¾Œã®å€¤ã§åŸ‹ã‚ã‚‹ï¼‰
                    df[column].fillna(method='bfill', inplace=True)
                    final_missing = df[column].isnull().sum()
                    
                    filled_count = original_missing - final_missing
                    
                    print(f"    âœ… å‰å¾Œè£œå®Œã§{filled_count:,}å€‹è£œå®Œå®Œäº†")
                    if final_missing > 0:
                        print(f"      âš ï¸  {final_missing}å€‹ã¯è£œå®Œã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    
                    datetime_missing_handled += 1
                    
                except Exception as e:
                    print(f"    âŒ è£œå®Œã‚¨ãƒ©ãƒ¼: {str(e)[:30]}...")
            else:
                print(f"    âŒ æ¬ æç‡ãŒé«˜ã„ãŸã‚è£œå®Œã‚’ã‚¹ã‚­ãƒƒãƒ—")
    
    print(f"  ğŸ“ˆ æ—¥ä»˜åˆ—å‡¦ç†çµæœ: {datetime_missing_handled}åˆ—å‡¦ç†å®Œäº†")
    
    # å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼
    missing_after_dict = df.isnull().sum().to_dict()
    total_missing_after = df.isnull().sum().sum()
    total_filled = total_missing_before - total_missing_after
    
    print(f"\nğŸ“Š æ¬ æå€¤å‡¦ç†ã®ã‚µãƒãƒªãƒ¼:")
    print(f"  ğŸ¯ ç·å‡¦ç†åˆ—æ•°: {numeric_missing_handled + categorical_missing_handled + datetime_missing_handled}åˆ—")
    print(f"    â”œâ”€ æ•°å€¤åˆ—: {numeric_missing_handled}åˆ—")
    print(f"    â”œâ”€ ã‚«ãƒ†ã‚´ãƒªåˆ—: {categorical_missing_handled}åˆ—")
    print(f"    â””â”€ æ—¥ä»˜åˆ—: {datetime_missing_handled}åˆ—")
    print(f"  ğŸ“ˆ æ¬ æå€¤ã®å¤‰åŒ–:")
    print(f"    â”œâ”€ å‡¦ç†å‰: {total_missing_before:,}å€‹")
    print(f"    â”œâ”€ å‡¦ç†å¾Œ: {total_missing_after:,}å€‹")
    print(f"    â”œâ”€ è£œå®Œæ•°: {total_filled:,}å€‹")
    print(f"    â””â”€ è£œå®Œç‡: {(total_filled/total_missing_before)*100:.1f}%" if total_missing_before > 0 else "    â””â”€ è£œå®Œç‡: N/A")
    
    # æ®‹å­˜æ¬ æå€¤ãŒã‚ã‚‹åˆ—ã®å ±å‘Š
    if total_missing_after > 0:
        print(f"\nâš ï¸  æ®‹å­˜æ¬ æå€¤ãŒã‚ã‚‹åˆ—:")
        for column in df.columns:
            remaining_missing = df[column].isnull().sum()
            if remaining_missing > 0:
                missing_rate = remaining_missing / len(df)
                print(f"    '{column}': {remaining_missing:,}å€‹ ({missing_rate*100:.1f}%)")
    else:
        print(f"\nğŸ‰ å…¨ã¦ã®æ¬ æå€¤ãŒå‡¦ç†ã•ã‚Œã¾ã—ãŸï¼")

logger.info(f"æ¬ æå€¤å‡¦ç†å®Œäº†: {total_filled}å€‹è£œå®Œã€{total_missing_after}å€‹æ®‹å­˜")

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³9: ç•°å¸¸å€¤ã®æ¤œå‡ºã¨å‡¦ç†ã€‘
# =============================================================================

print(f"\nğŸš¨ STEP 7: ç•°å¸¸å€¤ã®æ¤œå‡ºã¨å‡¦ç†")
print("-" * 50)

# æ•°å€¤åˆ—ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
numeric_columns = df.select_dtypes(include=[np.number]).columns

if len(numeric_columns) == 0:
    print(f"  â„¹ï¸  æ•°å€¤åˆ—ãŒãªã„ãŸã‚ã€ç•°å¸¸å€¤å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
else:
    print(f"ç•°å¸¸å€¤æ¤œå‡ºã®é–‹å§‹:")
    print(f"  ğŸ¯ å¯¾è±¡åˆ—æ•°: {len(numeric_columns)}åˆ—")
    
    outlier_summary = []  # ç•°å¸¸å€¤å‡¦ç†çµæœã‚’è¨˜éŒ²
    
    for column in numeric_columns:
        print(f"\nğŸ” '{column}' ã®ç•°å¸¸å€¤åˆ†æ:")
        
        # åŸºæœ¬çµ±è¨ˆã®ç¢ºèª
        valid_data = df[column].dropna()  # æ¬ æå€¤ã‚’é™¤å¤–
        
        if len(valid_data) == 0:
            print(f"    âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            continue
        
        # åŸºæœ¬çµ±è¨ˆé‡ã®è¡¨ç¤º
        print(f"    ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"      â”œâ”€ ä»¶æ•°: {len(valid_data):,}")
        print(f"      â”œâ”€ å¹³å‡: {valid_data.mean():.2f}")
        print(f"      â”œâ”€ ä¸­å¤®å€¤: {valid_data.median():.2f}")
        print(f"      â”œâ”€ æ¨™æº–åå·®: {valid_data.std():.2f}")
        print(f"      â”œâ”€ æœ€å°å€¤: {valid_data.min():.2f}")
        print(f"      â””â”€ æœ€å¤§å€¤: {valid_data.max():.2f}")
        
        # IQRæ³•ã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡º
        Q1 = valid_data.quantile(0.25)      # ç¬¬1å››åˆ†ä½æ•°
        Q3 = valid_data.quantile(0.75)      # ç¬¬3å››åˆ†ä½æ•°
        IQR = Q3 - Q1                       # å››åˆ†ä½ç¯„å›²
        
        # ç•°å¸¸å€¤ã®é–¾å€¤è¨ˆç®—
        lower_bound = Q1 - 1.5 * IQR        # ä¸‹é™
        upper_bound = Q3 + 1.5 * IQR        # ä¸Šé™
        
        print(f"    ğŸ¯ IQRç•°å¸¸å€¤æ¤œå‡º:")
        print(f"      â”œâ”€ Q1 (25%): {Q1:.2f}")
        print(f"      â”œâ”€ Q3 (75%): {Q3:.2f}")
        print(f"      â”œâ”€ IQR: {IQR:.2f}")
        print(f"      â”œâ”€ ä¸‹é™é–¾å€¤: {lower_bound:.2f}")
        print(f"      â””â”€ ä¸Šé™é–¾å€¤: {upper_bound:.2f}")
        
        # ç•°å¸¸å€¤ã®ç‰¹å®š
        outlier_mask = (df[column] < lower_bound) | (df[column] > upper_bound)
        outliers = df[outlier_mask][column].dropna()
        outlier_count = len(outliers)
        
        if outlier_count > 0:
            outlier_percentage = (outlier_count / len(valid_data)) * 100
            print(f"    ğŸ”´ ç•°å¸¸å€¤æ¤œå‡º: {outlier_count:,}å€‹ ({outlier_percentage:.2f}%)")
            
            # ç•°å¸¸å€¤ã®è©³ç´°åˆ†æ
            lower_outliers = df[df[column] < lower_bound][column].dropna()
            upper_outliers = df[df[column] > upper_bound][column].dropna()
            
            print(f"      â”œâ”€ ä¸‹é™è¶…é: {len(lower_outliers):,}å€‹")
            if len(lower_outliers) > 0:
                print(f"      â”‚   â””â”€ æœ€å°å€¤: {lower_outliers.min():.2f}")
            
            print(f"      â””â”€ ä¸Šé™è¶…é: {len(upper_outliers):,}å€‹")
            if len(upper_outliers) > 0:
                print(f"          â””â”€ æœ€å¤§å€¤: {upper_outliers.max():.2f}")
            
            # ç•°å¸¸å€¤ã®ä¾‹ã‚’è¡¨ç¤ºï¼ˆæœ€å¤§5å€‹ï¼‰
            if outlier_count <= 10:
                print(f"      ğŸ“‹ ç•°å¸¸å€¤ä¸€è¦§: {sorted(outliers.tolist())}")
            else:
                sample_outliers = sorted(outliers.tolist())[:5]
                print(f"      ğŸ“‹ ç•°å¸¸å€¤ä¾‹(5å€‹): {sample_outliers}...")
            
            # ç•°å¸¸å€¤å‡¦ç†ã®æ±ºå®š
            if outlier_percentage <= 5.0:  # 5%ä»¥ä¸‹ã®å ´åˆã¯å‡¦ç†ã‚’å®Ÿè¡Œ
                print(f"    ğŸ”§ ç•°å¸¸å€¤å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™...")
                
                # å‡¦ç†æ–¹æ³•ã®é¸æŠ
                if IQR > 0:  # IQRãŒ0ã§ãªã„å ´åˆï¼ˆãƒ‡ãƒ¼ã‚¿ã«ã°ã‚‰ã¤ããŒã‚ã‚‹ï¼‰
                    # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆé–¾å€¤ã§åˆ¶é™ï¼‰
                    original_values = df[column].copy()
                    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
                    
                    # å‡¦ç†çµæœã®ç¢ºèª
                    clipped_count = (original_values != df[column]).sum()
                    print(f"      âœ… ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å®Œäº†: {clipped_count:,}å€¤ã‚’ä¿®æ­£")
                    
                    outlier_summary.append({
                        'column': column,
                        'outlier_count': outlier_count,
                        'outlier_percentage': outlier_percentage,
                        'method': 'ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°',
                        'processed_count': clipped_count
                    })
                else:
                    print(f"      âš ï¸  ãƒ‡ãƒ¼ã‚¿ã«ã°ã‚‰ã¤ããŒãªã„ãŸã‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            else:
                print(f"    âš ï¸  ç•°å¸¸å€¤ãŒå¤šã™ãã‚‹ãŸã‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ— (>{outlier_percentage:.1f}%)")
                print(f"       ğŸ’¡ æ‰‹å‹•ã§ã®ç¢ºèªã‚’ãŠå‹§ã‚ã—ã¾ã™")
                
                outlier_summary.append({
                    'column': column,
                    'outlier_count': outlier_count,
                    'outlier_percentage': outlier_percentage,
                    'method': 'ã‚¹ã‚­ãƒƒãƒ—',
                    'processed_count': 0
                })
        else:
            print(f"    âœ… ç•°å¸¸å€¤ãªã—")
            outlier_summary.append({
                'column': column,
                'outlier_count': 0,
                'outlier_percentage': 0.0,
                'method': 'å‡¦ç†ä¸è¦',
                'processed_count': 0
            })
    
    # ç•°å¸¸å€¤å‡¦ç†ã®ã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“Š ç•°å¸¸å€¤å‡¦ç†ã®ã‚µãƒãƒªãƒ¼:")
    total_outliers_found = sum([item['outlier_count'] for item in outlier_summary])
    total_outliers_processed = sum([item['processed_count'] for item in outlier_summary])
    processed_columns = len([item for item in outlier_summary if item['processed_count'] > 0])
    
    print(f"  ğŸ¯ å‡¦ç†çµæœ:")
    print(f"    â”œâ”€ æ¤œæŸ»åˆ—æ•°: {len(numeric_columns)}åˆ—")
    print(f"    â”œâ”€ ç•°å¸¸å€¤ç™ºè¦‹æ•°: {total_outliers_found:,}å€‹")
    print(f"    â”œâ”€ å‡¦ç†æ¸ˆã¿ç•°å¸¸å€¤: {total_outliers_processed:,}å€‹")
    print(f"    â””â”€ å‡¦ç†åˆ—æ•°: {processed_columns}åˆ—")
    
    # åˆ—åˆ¥ã®è©³ç´°çµæœ
    if outlier_summary:
        print(f"\nğŸ“‹ åˆ—åˆ¥å‡¦ç†çµæœ:")
        for item in outlier_summary:
            if item['outlier_count'] > 0:
                print(f"    '{item['column']}': {item['outlier_count']:,}å€‹ç•°å¸¸å€¤ â†’ {item['method']}")

logger.info(f"ç•°å¸¸å€¤å‡¦ç†å®Œäº†: {total_outliers_processed}å€‹å‡¦ç†")

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³10: Tableauæœ€é©åŒ–å‡¦ç†ã€‘
# =============================================================================

print(f"\nâš¡ STEP 8: Tableauç”¨ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–")
print("-" * 50)

tableau_optimizations = []  # æœ€é©åŒ–å†…å®¹ã‚’è¨˜éŒ²

print(f"Tableauæœ€é©åŒ–ã®é–‹å§‹:")

# 1. æ—¥ä»˜åˆ—ã‹ã‚‰ã®æ™‚é–“è»¸åˆ—ç”Ÿæˆ
print(f"\nğŸ“… æ—¥ä»˜åˆ—ã‹ã‚‰æ™‚é–“è»¸åˆ—ã®ç”Ÿæˆ:")
datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns
date_columns_added = 0

for column in datetime_columns:
    print(f"  ğŸ” '{column}' ã‹ã‚‰æ™‚é–“è»¸åˆ—ã‚’ç”Ÿæˆä¸­...")
    
    # ãƒ™ãƒ¼ã‚¹åã®ç”Ÿæˆï¼ˆ_date, _timeã‚’é™¤å»ï¼‰
    base_name = column.replace('_date', '').replace('_time', '').replace('date_', '').replace('time_', '')
    if base_name == column:  # å¤‰æ›´ãŒãªã„å ´åˆã¯çŸ­ç¸®å½¢ã‚’ä½œæˆ
        base_name = column[:10] if len(column) > 10 else column
    
    try:
        # å¹´åˆ—
        year_col = f'{base_name}_year'
        df[year_col] = df[column].dt.year
        print(f"    âœ… {year_col}")
        date_columns_added += 1
        
        # æœˆåˆ—ï¼ˆæ•°å€¤ï¼‰
        month_col = f'{base_name}_month'
        df[month_col] = df[column].dt.month
        print(f"    âœ… {month_col}")
        date_columns_added += 1
        
        # æœˆååˆ—ï¼ˆæ–‡å­—åˆ—ï¼‰
        month_name_col = f'{base_name}_month_name'
        df[month_name_col] = df[column].dt.month_name()
        print(f"    âœ… {month_name_col}")
        date_columns_added += 1
        
        # å››åŠæœŸåˆ—
        quarter_col = f'{base_name}_quarter'
        df[quarter_col] = df[column].dt.quarter
        print(f"    âœ… {quarter_col}")
        date_columns_added += 1
        
        # æ›œæ—¥åˆ—ï¼ˆæ•°å€¤: æœˆæ›œ=0ï¼‰
        weekday_col = f'{base_name}_weekday'
        df[weekday_col] = df[column].dt.weekday
        print(f"    âœ… {weekday_col}")
        date_columns_added += 1
        
        # æ›œæ—¥ååˆ—ï¼ˆæ–‡å­—åˆ—ï¼‰
        weekday_name_col = f'{base_name}_weekday_name'
        df[weekday_name_col] = df[column].dt.day_name()
        print(f"    âœ… {weekday_name_col}")
        date_columns_added += 1
        
        # æ—¥åˆ—
        day_col = f'{base_name}_day'
        df[day_col] = df[column].dt.day
        print(f"    âœ… {day_col}")
        date_columns_added += 1
        
        # å¹´æœˆåˆ—ï¼ˆæ–‡å­—åˆ—: YYYY-MMå½¢å¼ï¼‰
        year_month_col = f'{base_name}_year_month'
        df[year_month_col] = df[column].dt.strftime('%Y-%m')
        print(f"    âœ… {year_month_col}")
        date_columns_added += 1
        
        # ä¼šè¨ˆå¹´åº¦åˆ—ï¼ˆ4æœˆå§‹ã¾ã‚Šï¼‰
        fiscal_year_col = f'{base_name}_fiscal_year'
        df[fiscal_year_col] = df[column].apply(
            lambda x: x.year if x.month >= 4 else x.year - 1 if pd.notna(x) else np.nan
        )
        print(f"    âœ… {fiscal_year_col} (4æœˆå§‹ã¾ã‚Š)")
        date_columns_added += 1
        
        tableau_optimizations.append(f"æ—¥ä»˜å±•é–‹: {column} â†’ 9å€‹ã®æ™‚é–“è»¸åˆ—")
        
    except Exception as e:
        print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...")

print(f"  ğŸ“ˆ æ™‚é–“è»¸åˆ—ç”Ÿæˆçµæœ: {date_columns_added}åˆ—è¿½åŠ ")

# 2. ã‚«ãƒ†ã‚´ãƒªå‹ã¸ã®æœ€é©åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªå‹æœ€é©åŒ–:")
category_optimizations = 0
memory_saved_total = 0

string_columns = df.select_dtypes(include=['object']).columns

for column in string_columns:
    unique_count = df[column].nunique()
    total_count = len(df[column])
    unique_ratio = unique_count / total_count if total_count > 0 else 0
    
    # ã‚«ãƒ†ã‚´ãƒªåŒ–ã®æ¡ä»¶ï¼š
    # 1. ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡ãŒ50%æœªæº€
    # 2. ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ãŒ1000å€‹æœªæº€
    # 3. ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹
    if unique_ratio < 0.5 and unique_count < 1000 and total_count > 0:
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¸¬å®š
        memory_before = df[column].memory_usage(deep=True) / 1024 / 1024
        
        # ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›
        df[column] = df[column].astype('category')
        
        memory_after = df[column].memory_usage(deep=True) / 1024 / 1024
        memory_saved = memory_before - memory_after
        memory_saved_total += memory_saved
        
        print(f"  âœ… '{column}': ã‚«ãƒ†ã‚´ãƒªåŒ–å®Œäº†")
        print(f"    ğŸ“Š {unique_count:,}ã‚«ãƒ†ã‚´ãƒª ({unique_ratio*100:.1f}%)")
        print(f"    ğŸ’¾ ãƒ¡ãƒ¢ãƒªç¯€ç´„: {memory_saved:.2f} MB")
        
        category_optimizations += 1
        tableau_optimizations.append(f"ã‚«ãƒ†ã‚´ãƒªåŒ–: {column} ({memory_saved:.1f}MBç¯€ç´„)")

print(f"  ğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªæœ€é©åŒ–çµæœ: {category_optimizations}åˆ—ã€{memory_saved_total:.2f}MBç¯€ç´„")

# 3. æ•°å€¤åˆ—ã®ç²¾åº¦æœ€é©åŒ–
print(f"\nğŸ”¢ æ•°å€¤åˆ—ã®ç²¾åº¦æœ€é©åŒ–:")
numeric_optimizations = 0
numeric_memory_saved = 0

numeric_columns = df.select_dtypes(include=[np.number]).columns

for column in numeric_columns:
    if df[column].notna().any():  # æœ‰åŠ¹ãªå€¤ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
        memory_before = df[column].memory_usage(deep=True) / 1024 / 1024
        original_dtype = df[column].dtype
        
        try:
            # æ•´æ•°åˆ—ã®æœ€é©åŒ–
            if df[column].dropna().apply(lambda x: x == int(x)).all():
                min_val = df[column].min()
                max_val = df[column].max()
                
                # æœ€é©ãªæ•´æ•°å‹ã‚’é¸æŠ
                if min_val >= 0:  # éè² æ•´æ•°
                    if max_val <= 255:
                        optimal_dtype = 'uint8'
                    elif max_val <= 65535:
                        optimal_dtype = 'uint16'
                    elif max_val <= 4294967295:
                        optimal_dtype = 'uint32'
                    else:
                        optimal_dtype = 'uint64'
                else:  # ç¬¦å·ä»˜ãæ•´æ•°
                    if min_val >= -128 and max_val <= 127:
                        optimal_dtype = 'int8'
                    elif min_val >= -32768 and max_val <= 32767:
                        optimal_dtype = 'int16'
                    elif min_val >= -2147483648 and max_val <= 2147483647:
                        optimal_dtype = 'int32'
                    else:
                        optimal_dtype = 'int64'
                
                # å‹å¤‰æ›å®Ÿè¡Œ
                if optimal_dtype != str(original_dtype):
                    df[column] = df[column].astype(optimal_dtype)
                    memory_after = df[column].memory_usage(deep=True) / 1024 / 1024
                    memory_saved = memory_before - memory_after
                    
                    if memory_saved > 0:
                        print(f"  âœ… '{column}': {original_dtype} â†’ {optimal_dtype}")
                        print(f"    ğŸ’¾ ãƒ¡ãƒ¢ãƒªç¯€ç´„: {memory_saved:.3f} MB")
                        numeric_optimizations += 1
                        numeric_memory_saved += memory_saved
                        tableau_optimizations.append(f"æ•°å€¤æœ€é©åŒ–: {column} ({memory_saved:.2f}MBç¯€ç´„)")
            
            # æµ®å‹•å°æ•°ç‚¹æ•°ã®æœ€é©åŒ–
            elif str(original_dtype) == 'float64':
                # float32ã§ç²¾åº¦ãŒä¿ãŸã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                test_float32 = df[column].astype('float32')
                if df[column].equals(test_float32.astype('float64')):
                    df[column] = test_float32
                    memory_after = df[column].memory_usage(deep=True) / 1024 / 1024
                    memory_saved = memory_before - memory_after
                    
                    print(f"  âœ… '{column}': float64 â†’ float32")
                    print(f"    ğŸ’¾ ãƒ¡ãƒ¢ãƒªç¯€ç´„: {memory_saved:.3f} MB")
                    numeric_optimizations += 1
                    numeric_memory_saved += memory_saved
                    tableau_optimizations.append(f"æµ®å‹•å°æ•°ç‚¹æœ€é©åŒ–: {column} ({memory_saved:.2f}MBç¯€ç´„)")
        
        except Exception as e:
            print(f"  âš ï¸  '{column}': æœ€é©åŒ–ã‚¹ã‚­ãƒƒãƒ— - {str(e)[:30]}...")

print(f"  ğŸ“ˆ æ•°å€¤æœ€é©åŒ–çµæœ: {numeric_optimizations}åˆ—ã€{numeric_memory_saved:.3f}MBç¯€ç´„")

# 4. ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³æ›¿ãˆï¼ˆTableauè¡¨ç¤ºé †æœ€é©åŒ–ï¼‰
print(f"\nğŸ”„ ãƒ‡ãƒ¼ã‚¿ä¸¦ã³æ›¿ãˆæœ€é©åŒ–:")

# æ—¥ä»˜åˆ—ãŒã‚ã‚‹å ´åˆã€æœ€æ–°æ—¥ä»˜é †ã«ä¸¦ã³æ›¿ãˆ
datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns
if len(datetime_columns) > 0:
    # æœ€åˆã®æ—¥ä»˜åˆ—ã§é™é †ã‚½ãƒ¼ãƒˆ
    primary_date_col = datetime_columns[0]
    df = df.sort_values(by=primary_date_col, ascending=False, na_position='last')
    print(f"  âœ… '{primary_date_col}'ã§é™é †ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°ãƒ‡ãƒ¼ã‚¿ãŒä¸Šä½ï¼‰")
    tableau_optimizations.append(f"ä¸¦ã³æ›¿ãˆ: {primary_date_col}ã§é™é †")
else:
    print(f"  â„¹ï¸  æ—¥ä»˜åˆ—ãŒãªã„ãŸã‚ã€ä¸¦ã³æ›¿ãˆã‚’ã‚¹ã‚­ãƒƒãƒ—")

# 5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ
print(f"\nğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ:")
df = df.reset_index(drop=True)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’0ã‹ã‚‰é€£ç•ªã«æŒ¯ã‚Šç›´ã—
print(f"  âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’0ã‹ã‚‰é€£ç•ªã«ãƒªã‚»ãƒƒãƒˆ")

# æœ€é©åŒ–çµæœã®ã‚µãƒãƒªãƒ¼
print(f"\nğŸ“Š Tableauæœ€é©åŒ–ã®ã‚µãƒãƒªãƒ¼:")
total_memory_saved = memory_saved_total + numeric_memory_saved
total_columns_added = date_columns_added
total_optimizations = category_optimizations + numeric_optimizations

print(f"  ğŸ¯ æœ€é©åŒ–çµæœ:")
print(f"    â”œâ”€ æ–°è¦è¿½åŠ åˆ—: {total_columns_added}åˆ—")
print(f"    â”œâ”€ æœ€é©åŒ–åˆ—æ•°: {total_optimizations}åˆ—")
print(f"    â”œâ”€ ç·ãƒ¡ãƒ¢ãƒªç¯€ç´„: {total_memory_saved:.2f} MB")
print(f"    â””â”€ æœ€çµ‚åˆ—æ•°: {len(df.columns)}åˆ—")

# æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶
final_shape = df.shape
print(f"  ğŸ“ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {final_shape[0]:,}è¡Œ Ã— {final_shape[1]}åˆ—")

# å®Ÿè¡Œã•ã‚ŒãŸæœ€é©åŒ–ã®è©³ç´°
if tableau_optimizations:
    print(f"\nğŸ“‹ å®Ÿè¡Œã•ã‚ŒãŸæœ€é©åŒ–:")
    for optimization in tableau_optimizations:
        print(f"    â€¢ {optimization}")

logger.info(f"Tableauæœ€é©åŒ–å®Œäº†: {total_columns_added}åˆ—è¿½åŠ ã€{total_memory_saved:.2f}MBç¯€ç´„")

# =============================================================================
# ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³11: æœ€çµ‚æ¤œè¨¼ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€‘
# =============================================================================

print(f"\nğŸ‰ STEP 9: æœ€çµ‚æ¤œè¨¼ã¨ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
print("-" * 50)

# å‡¦ç†çµ‚äº†æ™‚åˆ»ã®è¨˜éŒ²
end_time = datetime.now()
processing_time = end_time - start_time

print(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†ã®å®Œäº†:")
print(f"  â±ï¸  å‡¦ç†æ™‚é–“: {processing_time}")
print(f"  ğŸ“… é–‹å§‹æ™‚åˆ»: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"  ğŸ“… çµ‚äº†æ™‚åˆ»: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")

# æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã®å“è³ªç¢ºèª
print(f"\nğŸ” æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯:")

# 1. åŸºæœ¬çµ±è¨ˆ
final_rows, final_columns = df.shape
final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
final_missing = df.isnull().sum().sum()

print(f"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
print(f"    â”œâ”€ è¡Œæ•°: {final_rows:,}")
print(f"    â”œâ”€ åˆ—æ•°: {final_columns}")
print(f"    â”œâ”€ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory:.2f} MB")
print(f"    â””â”€ æ®‹å­˜æ¬ æå€¤: {final_missing:,}å€‹")

# 2. ãƒ‡ãƒ¼ã‚¿å‹ã®åˆ†å¸ƒ
print(f"\n  ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å‹åˆ†å¸ƒ:")
final_dtype_counts = df.dtypes.value_counts()
for dtype, count in final_dtype_counts.items():
    print(f"    {str(dtype):<15}: {count:>3}åˆ—")

# 3. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ç¢ºèª
if initial_memory > 0:  # åˆæœŸãƒ¡ãƒ¢ãƒªãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆ
    memory_change = initial_memory - final_memory
    memory_efficiency = (memory_change / initial_memory) * 100 if initial_memory > 0 else 0
    
    print(f"\n  ğŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡:")
    print(f"    â”œâ”€ åˆæœŸ: {initial_memory:.2f} MB")
    print(f"    â”œâ”€ æœ€çµ‚: {final_memory:.2f} MB")
    if memory_change > 0:
        print(f"    â”œâ”€ å‰Šæ¸›: {memory_change:.2f} MB ({memory_efficiency:.1f}%)")
        print(f"    â””â”€ åŠ¹ç‡: âœ… æœ€é©åŒ–æˆåŠŸ")
    else:
        print(f"    â”œâ”€ å¢—åŠ : {abs(memory_change):.2f} MB")
        print(f"    â””â”€ åŠ¹ç‡: âš ï¸  ãƒ¡ãƒ¢ãƒªå¢—åŠ ï¼ˆæ©Ÿèƒ½è¿½åŠ ã«ã‚ˆã‚‹ï¼‰")

# 4. Tableaué©æ€§ãƒã‚§ãƒƒã‚¯
print(f"\n  ğŸ¯ Tableaué©æ€§ãƒã‚§ãƒƒã‚¯:")

tableau_readiness_score = 0
max_score = 6

# æ—¥ä»˜åˆ—ã®å­˜åœ¨
datetime_columns = df.select_dtypes(include=['datetime64[ns]']).columns
if len(datetime_columns) > 0:
    print(f"    âœ… æ—¥ä»˜åˆ—: {len(datetime_columns)}åˆ—å­˜åœ¨")
    tableau_readiness_score += 1
else:
    print(f"    âš ï¸  æ—¥ä»˜åˆ—: ãªã—")

# æ•°å€¤åˆ—ã®å­˜åœ¨
numeric_columns = df.select_dtypes(include=[np.number]).columns
if len(numeric_columns) > 0:
    print(f"    âœ… æ•°å€¤åˆ—: {len(numeric_columns)}åˆ—å­˜åœ¨")
    tableau_readiness_score += 1
else:
    print(f"    âš ï¸  æ•°å€¤åˆ—: ãªã—")

# ã‚«ãƒ†ã‚´ãƒªåˆ—ã®å­˜åœ¨
categorical_columns = df.select_dtypes(include=['object', 'category']).columns
if len(categorical_columns) > 0:
    print(f"    âœ… ã‚«ãƒ†ã‚´ãƒªåˆ—: {len(categorical_columns)}åˆ—å­˜åœ¨")
    tableau_readiness_score += 1
else:
    print(f"    âš ï¸  ã‚«ãƒ†ã‚´ãƒªåˆ—: ãªã—")

# æ¬ æå€¤ã®çŠ¶æ³
if final_missing == 0:
    print(f"    âœ… æ¬ æå€¤: ãªã—")
    tableau_readiness_score += 1
elif final_missing < final_rows * final_columns * 0.05:  # 5%æœªæº€
    print(f"    âœ… æ¬ æå€¤: å°‘é‡ ({final_missing:,}å€‹)")
    tableau_readiness_score += 1
else:
    print(f"    âš ï¸  æ¬ æå€¤: å¤šæ•° ({final_missing:,}å€‹)")

# åˆ—åã®æ¨™æº–åŒ–
non_standard_columns = []
for col in df.columns:
    if not col.islower() or ' ' in col or any(char in col for char in '!@#$%^&*()+=[]{}|;:,<>?'):
        non_standard_columns.append(col)

if len(non_standard_columns) == 0:
    print(f"    âœ… åˆ—å: æ¨™æº–åŒ–æ¸ˆã¿")
    tableau_readiness_score += 1
else:
    print(f"    âš ï¸  åˆ—å: {len(non_standard_columns)}åˆ—ãŒéæ¨™æº–")

# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
if final_rows > 0 and final_columns > 0:
    print(f"    âœ… ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: é©åˆ‡")
    tableau_readiness_score += 1
else:
    print(f"    âŒ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: å•é¡Œã‚ã‚Š")

# ã‚¹ã‚³ã‚¢ã®è¡¨ç¤º
readiness_percentage = (tableau_readiness_score / max_score) * 100
print(f"\n    ğŸ† Tableaué©æ€§ã‚¹ã‚³ã‚¢: {tableau_readiness_score}/{max_score} ({readiness_percentage:.0f}%)")

if readiness_percentage >= 80:
    print(f"    ğŸ‰ Tableauå°å…¥æº–å‚™å®Œäº†ï¼")
elif readiness_percentage >= 60:
    print(f"    âœ… åŸºæœ¬çš„ãªTableauåˆ©ç”¨ãŒå¯èƒ½")
elif readiness_percentage >= 40:
    print(f"    âš ï¸  è¿½åŠ ã®èª¿æ•´ãŒæ¨å¥¨ã•ã‚Œã¾ã™")
else:
    print(f"    âŒ å¤§å¹…ãªä¿®æ­£ãŒå¿…è¦ã§ã™")

# 5. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå‡¦ç†
print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ:")

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
base_filename = 'tableau_ready_data'

# CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆæœ€ã‚‚ä¸€èˆ¬çš„ï¼‰
csv_filename = f'{base_filename}_{timestamp}.csv'
try:
    df.to_csv(csv_filename, index=False, encoding='utf-8')
    csv_size = os.path.getsize(csv_filename) / 1024 / 1024  # MB
    print(f"  âœ… CSVå‡ºåŠ›å®Œäº†: {csv_filename}")
    print(f"    ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {csv_size:.2f} MB")
except Exception as e:
    print(f"  âŒ CSVå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...")

# Excelå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ï¼‰
if final_rows <= 1000000:  # 100ä¸‡è¡Œä»¥ä¸‹ã®å ´åˆã®ã¿ï¼ˆExcelã®åˆ¶é™è€ƒæ…®ï¼‰
    excel_filename = f'{base_filename}_{timestamp}.xlsx'
    try:
        df.to_excel(excel_filename, index=False, engine='openpyxl')
        excel_size = os.path.getsize(excel_filename) / 1024 / 1024
        print(f"  âœ… Excelå‡ºåŠ›å®Œäº†: {excel_filename}")
        print(f"    ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {excel_size:.2f} MB")
    except Exception as e:
        print(f"  âš ï¸  Excelå‡ºåŠ›ã‚¹ã‚­ãƒƒãƒ—: {str(e)[:50]}...")
else:
    print(f"  â„¹ï¸  Excelå‡ºåŠ›ã‚¹ã‚­ãƒƒãƒ—: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ ({final_rows:,}è¡Œ)")

# Parquetå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆå¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿æ¨å¥¨ï¼‰
if final_rows > 100000:  # 10ä¸‡è¡Œä»¥ä¸Šã®å ´åˆ
    parquet_filename = f'{base_filename}_{timestamp}.parquet'
    try:
        df.to_parquet(parquet_filename, index=False, compression='snappy')
        parquet_size = os.path.getsize(parquet_filename) / 1024 / 1024
        print(f"  âœ… Parquetå‡ºåŠ›å®Œäº†: {parquet_filename}")
        print(f"    ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {parquet_size:.2f} MB")
        print(f"    ğŸ’¡ å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚Parquetå½¢å¼ã‚’æ¨å¥¨")
    except Exception as e:
        print(f"  âš ï¸  Parquetå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...")

# ãƒ‡ãƒ¼ã‚¿è¾æ›¸ã®ç”Ÿæˆï¼ˆåˆ—ã®è©³ç´°æƒ…å ±ï¼‰
print(f"\nğŸ“š ãƒ‡ãƒ¼ã‚¿è¾æ›¸ã®ç”Ÿæˆ:")
dictionary_filename = f'data_dictionary_{timestamp}.txt'

try:
    with open(dictionary_filename, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ãƒ‡ãƒ¼ã‚¿è¾æ›¸ (Data Dictionary)\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {final_rows:,}è¡Œ Ã— {final_columns}åˆ—\n")
        f.write(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory:.2f} MB\n")
        f.write(f"æ¬ æå€¤: {final_missing:,}å€‹\n\n")
        
        f.write("åˆ—ã®è©³ç´°æƒ…å ±:\n")
        f.write("-" * 80 + "\n")
        
        for i, column in enumerate(df.columns, 1):
            f.write(f"\n{i:3d}. {column}\n")
            f.write(f"     ãƒ‡ãƒ¼ã‚¿å‹: {df[column].dtype}\n")
            f.write(f"     æ¬ æå€¤: {df[column].isnull().sum():,}å€‹\n")
            f.write(f"     ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: {df[column].nunique():,}å€‹\n")
            
            if df[column].dtype in ['object', 'category']:
                # ã‚«ãƒ†ã‚´ãƒªåˆ—ã®å ´åˆ
                if df[column].nunique() <= 10:
                    unique_values = df[column].unique()
                    f.write(f"     å€¤ä¸€è¦§: {list(unique_values)}\n")
                else:
                    top_values = df[column].value_counts().head(5)
                    f.write(f"     ä¸Šä½5å€¤: {dict(top_values)}\n")
            elif np.issubdtype(df[column].dtype, np.number):
                # æ•°å€¤åˆ—ã®å ´åˆ
                f.write(f"     æœ€å°å€¤: {df[column].min()}\n")
                f.write(f"     æœ€å¤§å€¤: {df[column].max()}\n")
                f.write(f"     å¹³å‡å€¤: {df[column].mean():.2f}\n")
            elif df[column].dtype == 'datetime64[ns]':
                # æ—¥ä»˜åˆ—ã®å ´åˆ
                f.write(f"     æœ€æ—©æ—¥: {df[column].min()}\n")
                f.write(f"     æœ€é…æ—¥: {df[column].max()}\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("å‡¦ç†å®Œäº†\n")
    
    print(f"  âœ… ãƒ‡ãƒ¼ã‚¿è¾æ›¸ç”Ÿæˆå®Œäº†: {dictionary_filename}")
    
except Exception as e:
    print(f"  âš ï¸  ãƒ‡ãƒ¼ã‚¿è¾æ›¸ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)[:50]}...")

# æœ€çµ‚ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
print(f"\n" + "=" * 60)
print(f"ğŸŠ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ã‚µãƒãƒªãƒ¼")
print(f"=" * 60)

print(f"ğŸ“Š å‡¦ç†çµæœ:")
print(f"  â”œâ”€ å‡¦ç†æ™‚é–“: {processing_time}")
print(f"  â”œâ”€ åˆæœŸãƒ‡ãƒ¼ã‚¿: {initial_rows:,}è¡Œ Ã— {len(original_columns)}åˆ—")
print(f"  â”œâ”€ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {final_rows:,}è¡Œ Ã— {final_columns}åˆ—")
print(f"  â”œâ”€ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: {final_memory:.2f} MB")
print(f"  â””â”€ å“è³ªã‚¹ã‚³ã‚¢: {readiness_percentage:.0f}%")

print(f"\nğŸ¯ Tableauå°å…¥æ¨å¥¨äº‹é …:")
if readiness_percentage >= 80:
    print(f"  ğŸš€ ã™ãã«Tableauã§ã®åˆ†æã‚’é–‹å§‹ã§ãã¾ã™ï¼")
    print(f"  ğŸ’¡ æ¨å¥¨: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„")
elif readiness_percentage >= 60:
    print(f"  âœ… åŸºæœ¬çš„ãªåˆ†æãŒå¯èƒ½ã§ã™")
    print(f"  ğŸ’¡ æ¨å¥¨: ç°¡å˜ãªãƒãƒ£ãƒ¼ãƒˆã‹ã‚‰ä½œæˆã—ã¦ãã ã•ã„")
else:
    print(f"  âš ï¸  è¿½åŠ ã®ãƒ‡ãƒ¼ã‚¿æ•´å‚™ãŒæ¨å¥¨ã•ã‚Œã¾ã™")
    print(f"  ğŸ’¡ æ¨å¥¨: æ¬ æå€¤ã‚„ç•°å¸¸å€¤ã®æ‰‹å‹•ç¢ºèªã‚’ã—ã¦ãã ã•ã„")

print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
print(f"  â€¢ ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿: {csv_filename}")
if 'excel_filename' in locals():
    print(f"  â€¢ Excelãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {excel_filename}")
if 'parquet_filename' in locals():
    print(f"  â€¢ é«˜åŠ¹ç‡å½¢å¼: {parquet_filename}")
print(f"  â€¢ ãƒ‡ãƒ¼ã‚¿è¾æ›¸: {dictionary_filename}")

print(f"\nğŸ‰ ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
print(f"=" * 60)

# ãƒ­ã‚°ã®æœ€çµ‚è¨˜éŒ²
logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œå…¨çµ‚äº†: {processing_time}, å“è³ªã‚¹ã‚³ã‚¢{readiness_percentage:.0f}%")

# å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
print(f"\nâœ¨ ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ç´ æ™´ã‚‰ã—ã„ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’ï¼âœ¨")
